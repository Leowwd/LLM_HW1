{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a164d4b3",
   "metadata": {},
   "source": [
    "# 1. N-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c7e1a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "139c9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filepath: str):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip().lower().split() for line in lines]\n",
    "\n",
    "dir_root = \"/home/leo/LLM_HW1/\"\n",
    "\n",
    "training_corpus = load_corpus(dir_root + \"train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d4fd5",
   "metadata": {},
   "source": [
    "## Build N-gram Language Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9cf58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n, smoothing=0.01):\n",
    "        \"\"\"\n",
    "        Initialize N-gram language model\n",
    "        Args:\n",
    "            n: The value of n for n-grams (2 for bigram, 3 for trigram)\n",
    "            smoothing: Smoothing parameter (Laplace/add-k smoothing)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.n_gram_counts = None\n",
    "        self.n_minus_1_gram_counts = None\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def n_gram_distribution(self, tokenized_corpus, n=2):\n",
    "\n",
    "        if n < 2:\n",
    "            raise ValueError(\"n must be larger than 1\")\n",
    "\n",
    "        n_minus_1_gram_counts = defaultdict(int)\n",
    "        n_gram_counts = defaultdict(int)\n",
    "\n",
    "        for sentence in tokenized_corpus:\n",
    "            if len(sentence) < n:\n",
    "                continue\n",
    "            \n",
    "            for i in range(len(sentence) - n + 1):\n",
    "                n_gram = tuple(sentence[i : i + n])\n",
    "                n_minus_1_gram = tuple(sentence[i : i + n - 1])\n",
    "                \n",
    "                n_minus_1_gram_counts[n_minus_1_gram] += 1\n",
    "                n_gram_counts[n_gram] += 1\n",
    "                \n",
    "        return n_minus_1_gram_counts, n_gram_counts\n",
    "\n",
    "    def train(self, tokenized_corpus):\n",
    "        \"\"\"Train the n-gram model on a tokenized corpus\"\"\"\n",
    "        print(f\"Training {self.n}-gram model...\")\n",
    "        \n",
    "        # Get n-gram counts\n",
    "        self.n_minus_1_gram_counts, self.n_gram_counts = self.n_gram_distribution(\n",
    "            tokenized_corpus, self.n\n",
    "        )\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for sentence in tokenized_corpus:\n",
    "            self.vocab.update(sentence)\n",
    "        \n",
    "        # Build context index for fast lookup\n",
    "        self.context_index = defaultdict(list)\n",
    "        for n_gram in self.n_gram_counts.keys():\n",
    "            context = n_gram[:-1]\n",
    "            word = n_gram[-1]\n",
    "            self.context_index[context].append(word)\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"{self.n-1}-gram types: {len(self.n_minus_1_gram_counts)}\")\n",
    "        print(f\"{self.n}-gram types: {len(self.n_gram_counts)}\")\n",
    "        \n",
    "    def get_probability(self, n_gram):\n",
    "        \"\"\"\n",
    "        Calculate the probability of an n-gram with smoothing\n",
    "        P(wn | w1...wn-1) = (count(w1...wn) + smoothing) / (count(w1...wn-1) + smoothing * V)\n",
    "        \"\"\"\n",
    "        if len(n_gram) != self.n:\n",
    "            raise ValueError(f\"Expected {self.n}-gram, got {len(n_gram)}-gram\")\n",
    "        \n",
    "        context = tuple(n_gram[:-1])  # n-1 gram (context)\n",
    "        \n",
    "        numerator = self.n_gram_counts.get(tuple(n_gram), 0) + self.smoothing\n",
    "        denominator = self.n_minus_1_gram_counts.get(context, 0) + self.smoothing * len(self.vocab)\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def predict_next_word(self, context, top_k=5):\n",
    "        \"\"\"\n",
    "        Predict the most likely next word given a context\n",
    "        Args:\n",
    "            context: tuple of n-1 words\n",
    "            top_k: return top k predictions\n",
    "        Returns:\n",
    "            List of (word, probability) tuples\n",
    "        \"\"\"\n",
    "        if len(context) != self.n - 1:\n",
    "            raise ValueError(f\"Context should have {self.n-1} words\")\n",
    "        \n",
    "        context = tuple(context)\n",
    "        candidates = []\n",
    "        \n",
    "        # Fast lookup using context index\n",
    "        if context in self.context_index:\n",
    "            for word in self.context_index[context]:\n",
    "                n_gram = context + (word,)\n",
    "                prob = self.get_probability(n_gram)\n",
    "                candidates.append((word, prob))\n",
    "        \n",
    "        # Sort by probability\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return candidates[:top_k]\n",
    "    \n",
    "    def generate_sentence(self, start_words, max_length=20):\n",
    "        \"\"\"\n",
    "        Generate a sentence starting with given words\n",
    "        Args:\n",
    "            start_words: List of starting words (should be n-1 words)\n",
    "            max_length: Maximum length of generated sentence\n",
    "            sampling: If True, sample from distribution; if False, take most likely\n",
    "        Returns:\n",
    "            Generated sentence as a list of words\n",
    "        \"\"\"\n",
    "        if len(start_words) < self.n - 1:\n",
    "            raise ValueError(f\"Need at least {self.n-1} starting words\")\n",
    "        \n",
    "        sentence = list(start_words)\n",
    "        \n",
    "        while len(sentence) < max_length:\n",
    "            context = tuple(sentence[-(self.n-1):])\n",
    "            candidates = self.predict_next_word(context, top_k=1)\n",
    "            \n",
    "            if not candidates:\n",
    "                # No predictions available, stop\n",
    "                break\n",
    "            \n",
    "            # Take the most likely word\n",
    "            next_word = candidates[0][0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "        \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398cffa",
   "metadata": {},
   "source": [
    "## (a) Train Models and Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcaa994",
   "metadata": {},
   "source": [
    "### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9cd054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test corpus size: 649918 sentences\n",
      "First 3 test sentences: [['test', 'for', 'doneness'], ['add', 'cabbage', 'and', 'carrots'], ['add', 'onion', 'and', 'cook', 'until', 'golden', 'brown']]\n"
     ]
    }
   ],
   "source": [
    "test_corpus = load_corpus(dir_root + \"test.txt\")\n",
    "print(f\"Test corpus size: {len(test_corpus)} sentences\")\n",
    "print(f\"First 3 test sentences: {test_corpus[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ff914",
   "metadata": {},
   "source": [
    "### Train Trigram Model (n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3665e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRIGRAM MODEL (n=3)\n",
      "============================================================\n",
      "Training 3-gram model...\n",
      "Vocabulary size: 63069\n",
      "2-gram types: 622841\n",
      "3-gram types: 2353735\n",
      "\n",
      "Training time: 31.33 seconds\n",
      "Vocabulary size: 63069\n",
      "2-gram types: 622841\n",
      "3-gram types: 2353735\n",
      "\n",
      "Training time: 31.33 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRIGRAM MODEL (n=3)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, smoothing=0.01)\n",
    "\n",
    "start_time = time.time()\n",
    "trigram_model.train(training_corpus)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining time: {train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a18a9",
   "metadata": {},
   "source": [
    "### Evaluate Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31579a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_accuracy(model, test_corpus):\n",
    "    \"\"\"\n",
    "    Calculate prediction accuracy on test set\n",
    "    For each n-gram in test sentences, predict the next word and check if it matches\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    print(f\"Processing {len(test_corpus)} sentences...\")\n",
    "    \n",
    "    for sent_idx, sentence in enumerate(test_corpus):\n",
    "        if len(sentence) < model.n:\n",
    "            continue\n",
    "        \n",
    "        if (sent_idx + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {sent_idx + 1}/{len(test_corpus)} sentences... \"\n",
    "                  f\"Current accuracy: {correct_predictions}/{total_predictions} \"\n",
    "                  f\"({100*correct_predictions/total_predictions if total_predictions > 0 else 0:.2f}%)\")\n",
    "        \n",
    "        for i in range(len(sentence) - model.n + 1):\n",
    "            # Get context and actual next word\n",
    "            context = tuple(sentence[i : i + model.n - 1])\n",
    "            actual_word = sentence[i + model.n - 1]\n",
    "            \n",
    "            # Predict next word\n",
    "            candidates = model.predict_next_word(context, top_k=1)\n",
    "            \n",
    "            if candidates:\n",
    "                predicted_word = candidates[0][0]\n",
    "                if predicted_word == actual_word:\n",
    "                    correct_predictions += 1\n",
    "            \n",
    "            total_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy, correct_predictions, total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49c6928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING TRIGRAM MODEL ON TEST SET\n",
      "============================================================\n",
      "Processing 649918 sentences...\n",
      "  Processed 20000/649918 sentences... Current accuracy: 34275/95386 (35.93%)\n",
      "  Processed 30000/649918 sentences... Current accuracy: 51234/142451 (35.97%)\n",
      "  Processed 40000/649918 sentences... Current accuracy: 68277/190249 (35.89%)\n",
      "  Processed 60000/649918 sentences... Current accuracy: 102420/286059 (35.80%)\n",
      "  Processed 70000/649918 sentences... Current accuracy: 119552/334190 (35.77%)\n",
      "  Processed 80000/649918 sentences... Current accuracy: 136416/381992 (35.71%)\n",
      "  Processed 90000/649918 sentences... Current accuracy: 153337/429745 (35.68%)\n",
      "  Processed 100000/649918 sentences... Current accuracy: 170482/477722 (35.69%)\n",
      "  Processed 110000/649918 sentences... Current accuracy: 187455/525291 (35.69%)\n",
      "  Processed 120000/649918 sentences... Current accuracy: 204461/572788 (35.70%)\n",
      "  Processed 130000/649918 sentences... Current accuracy: 221300/619988 (35.69%)\n",
      "  Processed 140000/649918 sentences... Current accuracy: 238667/668176 (35.72%)\n",
      "  Processed 150000/649918 sentences... Current accuracy: 255587/715843 (35.70%)\n",
      "  Processed 160000/649918 sentences... Current accuracy: 272758/763668 (35.72%)\n",
      "  Processed 170000/649918 sentences... Current accuracy: 289871/811563 (35.72%)\n",
      "  Processed 180000/649918 sentences... Current accuracy: 306837/859496 (35.70%)\n",
      "  Processed 190000/649918 sentences... Current accuracy: 324118/907289 (35.72%)\n",
      "  Processed 200000/649918 sentences... Current accuracy: 341201/955398 (35.71%)\n",
      "  Processed 210000/649918 sentences... Current accuracy: 358492/1003496 (35.72%)\n",
      "  Processed 220000/649918 sentences... Current accuracy: 375720/1051891 (35.72%)\n",
      "  Processed 230000/649918 sentences... Current accuracy: 392949/1100433 (35.71%)\n",
      "  Processed 240000/649918 sentences... Current accuracy: 409999/1147946 (35.72%)\n",
      "  Processed 250000/649918 sentences... Current accuracy: 427300/1196229 (35.72%)\n",
      "  Processed 260000/649918 sentences... Current accuracy: 444120/1243286 (35.72%)\n",
      "  Processed 280000/649918 sentences... Current accuracy: 478304/1338297 (35.74%)\n",
      "  Processed 290000/649918 sentences... Current accuracy: 495179/1385927 (35.73%)\n",
      "  Processed 300000/649918 sentences... Current accuracy: 512089/1433254 (35.73%)\n",
      "  Processed 310000/649918 sentences... Current accuracy: 529218/1480675 (35.74%)\n",
      "  Processed 320000/649918 sentences... Current accuracy: 546175/1527896 (35.75%)\n",
      "  Processed 330000/649918 sentences... Current accuracy: 563207/1575423 (35.75%)\n",
      "  Processed 340000/649918 sentences... Current accuracy: 580247/1623296 (35.74%)\n",
      "  Processed 360000/649918 sentences... Current accuracy: 614617/1719574 (35.74%)\n",
      "  Processed 370000/649918 sentences... Current accuracy: 631971/1768265 (35.74%)\n",
      "  Processed 390000/649918 sentences... Current accuracy: 666467/1864932 (35.74%)\n",
      "  Processed 400000/649918 sentences... Current accuracy: 683405/1912319 (35.74%)\n",
      "  Processed 420000/649918 sentences... Current accuracy: 717624/2008288 (35.73%)\n",
      "  Processed 430000/649918 sentences... Current accuracy: 734741/2056626 (35.73%)\n",
      "  Processed 440000/649918 sentences... Current accuracy: 751681/2104152 (35.72%)\n",
      "  Processed 450000/649918 sentences... Current accuracy: 768663/2151431 (35.73%)\n",
      "  Processed 460000/649918 sentences... Current accuracy: 785986/2199995 (35.73%)\n",
      "  Processed 470000/649918 sentences... Current accuracy: 802863/2247892 (35.72%)\n",
      "  Processed 480000/649918 sentences... Current accuracy: 820246/2296180 (35.72%)\n",
      "  Processed 490000/649918 sentences... Current accuracy: 837334/2344613 (35.71%)\n",
      "  Processed 500000/649918 sentences... Current accuracy: 854458/2392576 (35.71%)\n",
      "  Processed 510000/649918 sentences... Current accuracy: 871647/2441165 (35.71%)\n",
      "  Processed 520000/649918 sentences... Current accuracy: 888690/2488978 (35.71%)\n",
      "  Processed 530000/649918 sentences... Current accuracy: 905573/2535944 (35.71%)\n",
      "  Processed 540000/649918 sentences... Current accuracy: 922338/2583324 (35.70%)\n",
      "  Processed 550000/649918 sentences... Current accuracy: 939443/2630925 (35.71%)\n",
      "  Processed 560000/649918 sentences... Current accuracy: 956518/2678779 (35.71%)\n",
      "  Processed 570000/649918 sentences... Current accuracy: 973667/2726714 (35.71%)\n",
      "  Processed 580000/649918 sentences... Current accuracy: 990648/2774321 (35.71%)\n",
      "  Processed 590000/649918 sentences... Current accuracy: 1007878/2822633 (35.71%)\n",
      "  Processed 600000/649918 sentences... Current accuracy: 1024746/2869721 (35.71%)\n",
      "  Processed 610000/649918 sentences... Current accuracy: 1042030/2917430 (35.72%)\n",
      "  Processed 620000/649918 sentences... Current accuracy: 1059185/2965285 (35.72%)\n",
      "  Processed 630000/649918 sentences... Current accuracy: 1076255/3013219 (35.72%)\n",
      "  Processed 640000/649918 sentences... Current accuracy: 1093173/3060537 (35.72%)\n",
      "\n",
      "Trigram Model Results:\n",
      "  Correct Predictions: 1109931 / 3107523\n",
      "  Accuracy: 35.72%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Trigram Model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING TRIGRAM MODEL ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trigram_accuracy, trigram_correct, trigram_total = calculate_test_accuracy(trigram_model, test_corpus)\n",
    "print(f\"\\nTrigram Model Results:\")\n",
    "print(f\"  Correct Predictions: {trigram_correct} / {trigram_total}\")\n",
    "print(f\"  Accuracy: {trigram_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d675ad",
   "metadata": {},
   "source": [
    "## (b) Hardware Usage Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a612e7d",
   "metadata": {},
   "source": [
    "在training的時候發現memory佔比會比較高，並且都是用一個cpu跑的，但gpu卻是不會用到的，因為我們沒有用pytorch之類的gpu加速套件。\n",
    "但是速度還是非常快的。\n",
    "反之，testing的時候memory佔比高且時間非常的長。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e3f0d",
   "metadata": {},
   "source": [
    "## (c) Sentence Completion with Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3a40721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incomplete sentences: 10\n"
     ]
    }
   ],
   "source": [
    "# Load incomplete sentences\n",
    "incomplete_sentences = load_corpus(dir_root + \"incomplete.txt\")\n",
    "\n",
    "print(f\"Number of incomplete sentences: {len(incomplete_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e4d096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SENTENCE COMPLETION USING TRIGRAM MODEL\n",
      "================================================================================\n",
      "\n",
      "Sentence 0:\n",
      "  Incomplete: cover with\n",
      "  Completed:  cover with foil and bake for about 5 minutes or until the mixture into the pan and bake for about\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 1:\n",
      "  Incomplete: roll up\n",
      "  Completed:  roll up and place in a large bowl and mix well and set aside to cool completely on wire rack\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 2:\n",
      "  Incomplete: cook the\n",
      "  Completed:  cook the pasta and cook for about 5 minutes or until the mixture into the pan and bake for about\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 3:\n",
      "  Incomplete: stir in\n",
      "  Completed:  stir in the center of the pan and bake for about 5 minutes or until the mixture into the pan\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 4:\n",
      "  Incomplete: spread out\n",
      "  Completed:  spread out on a baking sheet and bake for about 5 minutes or until the mixture into the pan and\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 5:\n",
      "  Incomplete: transfer the\n",
      "  Completed:  transfer the mixture into the pan and bake for about 5 minutes or until the mixture into the pan and\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 6:\n",
      "  Incomplete: put the\n",
      "  Completed:  transfer the mixture into the pan and bake for about 5 minutes or until the mixture into the pan and\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 6:\n",
      "  Incomplete: put the\n",
      "  Completed:  put the chicken and cook for about 5 minutes or until the mixture into the pan and bake for about\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 7:\n",
      "  Incomplete: push the\n",
      "  Completed:  put the chicken and cook for about 5 minutes or until the mixture into the pan and bake for about\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 7:\n",
      "  Incomplete: push the\n",
      "  Completed:  push the dough into a large bowl and mix well and set aside to cool completely on wire rack to\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 8:\n",
      "  Incomplete: cut into\n",
      "  Completed:  cut into squares and serve with a fork and serve with a fork and serve with a fork and serve\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 9:\n",
      "  Incomplete: toss the\n",
      "  Completed:  toss the salad and toss to coat the bottom of the pan and bake for about 5 minutes or until\n",
      "  Length: 20 words\n",
      "\n",
      "  Completed:  push the dough into a large bowl and mix well and set aside to cool completely on wire rack to\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 8:\n",
      "  Incomplete: cut into\n",
      "  Completed:  cut into squares and serve with a fork and serve with a fork and serve with a fork and serve\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 9:\n",
      "  Incomplete: toss the\n",
      "  Completed:  toss the salad and toss to coat the bottom of the pan and bake for about 5 minutes or until\n",
      "  Length: 20 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete sentences using the trigram model\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SENTENCE COMPLETION USING TRIGRAM MODEL\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "completed_sentences = []\n",
    "\n",
    "for i, incomplete in enumerate(incomplete_sentences):\n",
    "    print(f\"Sentence {i}:\")\n",
    "    print(f\"  Incomplete: {' '.join(incomplete)}\")\n",
    "    \n",
    "    # Generate completion\n",
    "    completed = trigram_model.generate_sentence(incomplete, max_length=20)\n",
    "    completed_sentences.append(completed)\n",
    "    \n",
    "    print(f\"  Completed:  {' '.join(completed)}\")\n",
    "    print(f\"  Length: {len(completed)} words\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffb732",
   "metadata": {},
   "source": [
    "# 2. RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748647df",
   "metadata": {},
   "source": [
    "## Prepare Data for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ae85fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Vocabulary size: 28127\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Build vocabulary from training corpus\n",
    "def build_vocab(corpus, min_freq=2):\n",
    "    \"\"\"Build vocabulary from corpus\"\"\"\n",
    "    word_freq = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            word_freq[word] += 1\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2}\n",
    "    idx = 3\n",
    "    \n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary\n",
    "rnn_vocab = build_vocab(training_corpus, min_freq=2)\n",
    "vocab_size = len(rnn_vocab)\n",
    "print(f\"RNN Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7ef4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: 180758\n",
      "Test sequences: 44150\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset class\n",
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, seq_length=20):\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.data = []\n",
    "        \n",
    "        # Convert corpus to sequences\n",
    "        for sentence in corpus:\n",
    "            # Convert words to indices\n",
    "            indices = [vocab.get(word, vocab['<unk>']) for word in sentence]\n",
    "            indices.append(vocab['<eos>'])  # Add end of sentence token\n",
    "            \n",
    "            # Create sequences of fixed length\n",
    "            for i in range(len(indices) - 1):\n",
    "                if i + seq_length + 1 <= len(indices):\n",
    "                    # Input: seq_length words, Target: next seq_length words\n",
    "                    input_seq = indices[i:i+seq_length]\n",
    "                    target_seq = indices[i+1:i+seq_length+1]\n",
    "                    self.data.append((input_seq, target_seq))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.data[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "seq_length = 20\n",
    "train_dataset = LanguageModelDataset(training_corpus, rnn_vocab, seq_length=seq_length)\n",
    "test_dataset = LanguageModelDataset(test_corpus, rnn_vocab, seq_length=seq_length)\n",
    "\n",
    "print(f\"Training sequences: {len(train_dataset)}\")\n",
    "print(f\"Test sequences: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b2401d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 5649\n",
      "Number of test batches: 1380\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c937e",
   "metadata": {},
   "source": [
    "## Build RNN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "735a8ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNLanguageModel(\n",
      "  (embedding): Embedding(28127, 128)\n",
      "  (rnn): RNN(128, 128, num_layers=2)\n",
      "  (fc): Linear(in_features=128, out_features=28127, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "\n",
      "Total parameters: 7,294,687\n"
     ]
    }
   ],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=2, dropout=0.5):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        output, hidden = self.rnn(embedded, hidden)  # output: (batch_size, seq_length, hidden_dim)\n",
    "        \n",
    "        # Apply dropout and linear layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)  # (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "# Create RNN model\n",
    "rnn_model = RNNLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(rnn_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3ae16",
   "metadata": {},
   "source": [
    "## Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28ad4d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model ready for training!\n"
     ]
    }
   ],
   "source": [
    "def train_rnn(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the RNN model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(inputs)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        # output: (batch_size, seq_length, vocab_size)\n",
    "        # targets: (batch_size, seq_length)\n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_correct += (predicted == targets).sum().item()\n",
    "        total_words += targets.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'  Batch {batch_idx+1}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'Acc: {100*total_correct/total_words:.2f}%')\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = total_correct / total_words\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_rnn(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate the RNN model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, _ = model(inputs)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_words += targets.size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / total_words\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, accuracy, perplexity\n",
    "\n",
    "# Setup training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "rnn_model = rnn_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nModel ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "348014f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING RNN LANGUAGE MODEL\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 6.1783, Acc: 5.87%\n",
      "  Batch 100/5649, Loss: 6.1783, Acc: 5.87%\n",
      "  Batch 200/5649, Loss: 6.1769, Acc: 6.92%\n",
      "  Batch 200/5649, Loss: 6.1769, Acc: 6.92%\n",
      "  Batch 300/5649, Loss: 5.7661, Acc: 7.77%\n",
      "  Batch 300/5649, Loss: 5.7661, Acc: 7.77%\n",
      "  Batch 400/5649, Loss: 5.7882, Acc: 8.41%\n",
      "  Batch 400/5649, Loss: 5.7882, Acc: 8.41%\n",
      "  Batch 500/5649, Loss: 5.6704, Acc: 9.17%\n",
      "  Batch 500/5649, Loss: 5.6704, Acc: 9.17%\n",
      "  Batch 600/5649, Loss: 5.5194, Acc: 9.98%\n",
      "  Batch 600/5649, Loss: 5.5194, Acc: 9.98%\n",
      "  Batch 700/5649, Loss: 5.0138, Acc: 10.83%\n",
      "  Batch 700/5649, Loss: 5.0138, Acc: 10.83%\n",
      "  Batch 800/5649, Loss: 5.1558, Acc: 11.63%\n",
      "  Batch 800/5649, Loss: 5.1558, Acc: 11.63%\n",
      "  Batch 900/5649, Loss: 5.0693, Acc: 12.37%\n",
      "  Batch 900/5649, Loss: 5.0693, Acc: 12.37%\n",
      "  Batch 1000/5649, Loss: 4.9950, Acc: 13.00%\n",
      "  Batch 1000/5649, Loss: 4.9950, Acc: 13.00%\n",
      "  Batch 1100/5649, Loss: 4.9811, Acc: 13.56%\n",
      "  Batch 1100/5649, Loss: 4.9811, Acc: 13.56%\n",
      "  Batch 1200/5649, Loss: 4.9578, Acc: 14.09%\n",
      "  Batch 1200/5649, Loss: 4.9578, Acc: 14.09%\n",
      "  Batch 1300/5649, Loss: 4.8125, Acc: 14.53%\n",
      "  Batch 1400/5649, Loss: 4.9603, Acc: 14.95%\n",
      "  Batch 1500/5649, Loss: 4.9825, Acc: 15.31%\n",
      "  Batch 1600/5649, Loss: 4.7536, Acc: 15.65%\n",
      "  Batch 1700/5649, Loss: 4.7701, Acc: 15.97%\n",
      "  Batch 1800/5649, Loss: 4.5016, Acc: 16.25%\n",
      "  Batch 1900/5649, Loss: 4.6673, Acc: 16.49%\n",
      "  Batch 2000/5649, Loss: 4.5848, Acc: 16.73%\n",
      "  Batch 2100/5649, Loss: 5.0004, Acc: 16.93%\n",
      "  Batch 2200/5649, Loss: 4.2881, Acc: 17.14%\n",
      "  Batch 2300/5649, Loss: 4.4838, Acc: 17.32%\n",
      "  Batch 2400/5649, Loss: 4.8440, Acc: 17.48%\n",
      "  Batch 2500/5649, Loss: 4.4897, Acc: 17.64%\n",
      "  Batch 2600/5649, Loss: 4.8235, Acc: 17.78%\n",
      "  Batch 2700/5649, Loss: 4.4714, Acc: 17.91%\n",
      "  Batch 2800/5649, Loss: 4.4877, Acc: 18.05%\n",
      "  Batch 2900/5649, Loss: 4.3640, Acc: 18.17%\n",
      "  Batch 3000/5649, Loss: 4.4414, Acc: 18.26%\n",
      "  Batch 3100/5649, Loss: 4.3368, Acc: 18.36%\n",
      "  Batch 3200/5649, Loss: 4.3169, Acc: 18.46%\n",
      "  Batch 3300/5649, Loss: 4.1001, Acc: 18.56%\n",
      "  Batch 3400/5649, Loss: 4.3616, Acc: 18.65%\n",
      "  Batch 3500/5649, Loss: 4.7238, Acc: 18.74%\n",
      "  Batch 3600/5649, Loss: 4.6135, Acc: 18.83%\n",
      "  Batch 3700/5649, Loss: 4.4685, Acc: 18.90%\n",
      "  Batch 3800/5649, Loss: 4.6536, Acc: 18.98%\n",
      "  Batch 3900/5649, Loss: 4.3949, Acc: 19.05%\n",
      "  Batch 4000/5649, Loss: 4.3898, Acc: 19.11%\n",
      "  Batch 4100/5649, Loss: 4.3134, Acc: 19.17%\n",
      "  Batch 4200/5649, Loss: 4.5924, Acc: 19.23%\n",
      "  Batch 4300/5649, Loss: 4.5637, Acc: 19.29%\n",
      "  Batch 4400/5649, Loss: 4.4870, Acc: 19.35%\n",
      "  Batch 4500/5649, Loss: 4.5196, Acc: 19.40%\n",
      "  Batch 4600/5649, Loss: 4.5444, Acc: 19.46%\n",
      "  Batch 4700/5649, Loss: 4.1360, Acc: 19.52%\n",
      "  Batch 4800/5649, Loss: 4.5080, Acc: 19.57%\n",
      "  Batch 4900/5649, Loss: 4.6589, Acc: 19.62%\n",
      "  Batch 5000/5649, Loss: 4.3498, Acc: 19.66%\n",
      "  Batch 5100/5649, Loss: 4.2518, Acc: 19.70%\n",
      "  Batch 5200/5649, Loss: 4.5611, Acc: 19.74%\n",
      "  Batch 5300/5649, Loss: 4.2264, Acc: 19.78%\n",
      "  Batch 5400/5649, Loss: 4.3749, Acc: 19.83%\n",
      "  Batch 5500/5649, Loss: 4.3292, Acc: 19.87%\n",
      "  Batch 5600/5649, Loss: 4.1928, Acc: 19.90%\n",
      "Train Loss: 4.7408, Train Accuracy: 19.92%\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.3140, Acc: 22.14%\n",
      "  Batch 200/5649, Loss: 4.5614, Acc: 22.02%\n",
      "  Batch 300/5649, Loss: 4.4051, Acc: 22.01%\n",
      "  Batch 400/5649, Loss: 4.3918, Acc: 22.16%\n",
      "  Batch 500/5649, Loss: 4.1998, Acc: 22.24%\n",
      "  Batch 600/5649, Loss: 4.2738, Acc: 22.23%\n",
      "  Batch 700/5649, Loss: 4.4791, Acc: 22.25%\n",
      "  Batch 800/5649, Loss: 4.0990, Acc: 22.25%\n",
      "  Batch 900/5649, Loss: 4.2578, Acc: 22.26%\n",
      "  Batch 1000/5649, Loss: 4.5366, Acc: 22.26%\n",
      "  Batch 1100/5649, Loss: 4.1104, Acc: 22.28%\n",
      "  Batch 1200/5649, Loss: 4.2154, Acc: 22.28%\n",
      "  Batch 1300/5649, Loss: 4.3696, Acc: 22.26%\n",
      "  Batch 1400/5649, Loss: 4.5350, Acc: 22.26%\n",
      "  Batch 1500/5649, Loss: 4.0382, Acc: 22.27%\n",
      "  Batch 1600/5649, Loss: 4.3469, Acc: 22.28%\n",
      "  Batch 1700/5649, Loss: 4.1447, Acc: 22.29%\n",
      "  Batch 1800/5649, Loss: 4.1838, Acc: 22.29%\n",
      "  Batch 1900/5649, Loss: 4.2089, Acc: 22.32%\n",
      "  Batch 2000/5649, Loss: 4.4214, Acc: 22.34%\n",
      "  Batch 2100/5649, Loss: 4.4046, Acc: 22.35%\n",
      "  Batch 2200/5649, Loss: 4.2500, Acc: 22.34%\n",
      "  Batch 2300/5649, Loss: 4.1766, Acc: 22.34%\n",
      "  Batch 2400/5649, Loss: 4.2741, Acc: 22.35%\n",
      "  Batch 2500/5649, Loss: 4.1191, Acc: 22.36%\n",
      "  Batch 2600/5649, Loss: 4.2278, Acc: 22.36%\n",
      "  Batch 2700/5649, Loss: 4.2579, Acc: 22.36%\n",
      "  Batch 2800/5649, Loss: 4.2201, Acc: 22.36%\n",
      "  Batch 2900/5649, Loss: 4.0861, Acc: 22.37%\n",
      "  Batch 3000/5649, Loss: 4.3483, Acc: 22.38%\n",
      "  Batch 3100/5649, Loss: 4.2586, Acc: 22.38%\n",
      "  Batch 3200/5649, Loss: 4.0324, Acc: 22.39%\n",
      "  Batch 3300/5649, Loss: 4.2891, Acc: 22.39%\n",
      "  Batch 3400/5649, Loss: 4.3927, Acc: 22.40%\n",
      "  Batch 3500/5649, Loss: 4.0507, Acc: 22.39%\n",
      "  Batch 3600/5649, Loss: 4.1484, Acc: 22.39%\n",
      "  Batch 3700/5649, Loss: 4.2539, Acc: 22.40%\n",
      "  Batch 3800/5649, Loss: 4.2397, Acc: 22.41%\n",
      "  Batch 3900/5649, Loss: 4.2532, Acc: 22.41%\n",
      "  Batch 4000/5649, Loss: 4.1623, Acc: 22.41%\n",
      "  Batch 4100/5649, Loss: 4.3802, Acc: 22.40%\n",
      "  Batch 4200/5649, Loss: 4.3255, Acc: 22.40%\n",
      "  Batch 4300/5649, Loss: 4.2208, Acc: 22.40%\n",
      "  Batch 4400/5649, Loss: 4.2910, Acc: 22.41%\n",
      "  Batch 4500/5649, Loss: 4.3457, Acc: 22.40%\n",
      "  Batch 4600/5649, Loss: 4.2251, Acc: 22.40%\n",
      "  Batch 4700/5649, Loss: 4.6254, Acc: 22.40%\n",
      "  Batch 4800/5649, Loss: 4.2789, Acc: 22.41%\n",
      "  Batch 4900/5649, Loss: 4.2016, Acc: 22.41%\n",
      "  Batch 5000/5649, Loss: 4.1731, Acc: 22.41%\n",
      "  Batch 5100/5649, Loss: 4.3379, Acc: 22.41%\n",
      "  Batch 5200/5649, Loss: 4.1532, Acc: 22.42%\n",
      "  Batch 5300/5649, Loss: 4.1728, Acc: 22.43%\n",
      "  Batch 5400/5649, Loss: 4.3780, Acc: 22.43%\n",
      "  Batch 5500/5649, Loss: 4.1006, Acc: 22.43%\n",
      "  Batch 5600/5649, Loss: 4.4367, Acc: 22.43%\n",
      "Train Loss: 4.2870, Train Accuracy: 22.44%\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 3.9997, Acc: 22.64%\n",
      "  Batch 200/5649, Loss: 4.5255, Acc: 22.56%\n",
      "  Batch 300/5649, Loss: 3.9970, Acc: 22.63%\n",
      "  Batch 400/5649, Loss: 4.2745, Acc: 22.69%\n",
      "  Batch 500/5649, Loss: 4.0953, Acc: 22.66%\n",
      "  Batch 600/5649, Loss: 4.1874, Acc: 22.67%\n",
      "  Batch 700/5649, Loss: 4.0922, Acc: 22.62%\n",
      "  Batch 800/5649, Loss: 4.0568, Acc: 22.64%\n",
      "  Batch 900/5649, Loss: 4.2854, Acc: 22.65%\n",
      "  Batch 1000/5649, Loss: 4.3117, Acc: 22.66%\n",
      "  Batch 1100/5649, Loss: 4.0885, Acc: 22.68%\n",
      "  Batch 1200/5649, Loss: 4.0178, Acc: 22.69%\n",
      "  Batch 1300/5649, Loss: 4.2962, Acc: 22.72%\n",
      "  Batch 1400/5649, Loss: 4.1032, Acc: 22.70%\n",
      "  Batch 1500/5649, Loss: 4.0481, Acc: 22.73%\n",
      "  Batch 1600/5649, Loss: 4.0749, Acc: 22.74%\n",
      "  Batch 1700/5649, Loss: 4.1830, Acc: 22.76%\n",
      "  Batch 1800/5649, Loss: 4.3361, Acc: 22.76%\n",
      "  Batch 1900/5649, Loss: 4.1798, Acc: 22.77%\n",
      "  Batch 2000/5649, Loss: 4.0065, Acc: 22.77%\n",
      "  Batch 2100/5649, Loss: 4.0410, Acc: 22.77%\n",
      "  Batch 2200/5649, Loss: 4.1764, Acc: 22.77%\n",
      "  Batch 2300/5649, Loss: 4.3626, Acc: 22.76%\n",
      "  Batch 2400/5649, Loss: 4.4334, Acc: 22.76%\n",
      "  Batch 2500/5649, Loss: 4.0560, Acc: 22.76%\n",
      "  Batch 2600/5649, Loss: 4.4322, Acc: 22.75%\n",
      "  Batch 2700/5649, Loss: 4.1146, Acc: 22.76%\n",
      "  Batch 2800/5649, Loss: 4.3962, Acc: 22.77%\n",
      "  Batch 2900/5649, Loss: 4.0513, Acc: 22.77%\n",
      "  Batch 3000/5649, Loss: 4.4209, Acc: 22.77%\n",
      "  Batch 3100/5649, Loss: 4.2263, Acc: 22.77%\n",
      "  Batch 3200/5649, Loss: 4.1273, Acc: 22.77%\n",
      "  Batch 3300/5649, Loss: 4.0523, Acc: 22.77%\n",
      "  Batch 3400/5649, Loss: 4.3893, Acc: 22.76%\n",
      "  Batch 3500/5649, Loss: 4.1232, Acc: 22.78%\n",
      "  Batch 3600/5649, Loss: 4.1113, Acc: 22.78%\n",
      "  Batch 3700/5649, Loss: 4.2983, Acc: 22.78%\n",
      "  Batch 3800/5649, Loss: 4.0056, Acc: 22.78%\n",
      "  Batch 3900/5649, Loss: 4.4378, Acc: 22.78%\n",
      "  Batch 4000/5649, Loss: 4.0928, Acc: 22.78%\n",
      "  Batch 4100/5649, Loss: 4.0397, Acc: 22.78%\n",
      "  Batch 4200/5649, Loss: 3.9434, Acc: 22.77%\n",
      "  Batch 4300/5649, Loss: 4.4465, Acc: 22.77%\n",
      "  Batch 4400/5649, Loss: 4.3287, Acc: 22.78%\n",
      "  Batch 4500/5649, Loss: 4.2236, Acc: 22.78%\n",
      "  Batch 4600/5649, Loss: 4.2160, Acc: 22.77%\n",
      "  Batch 4700/5649, Loss: 4.4895, Acc: 22.77%\n",
      "  Batch 4800/5649, Loss: 4.0867, Acc: 22.78%\n",
      "  Batch 4900/5649, Loss: 4.2719, Acc: 22.78%\n",
      "  Batch 5000/5649, Loss: 4.3211, Acc: 22.79%\n",
      "  Batch 5100/5649, Loss: 4.0951, Acc: 22.79%\n",
      "  Batch 5200/5649, Loss: 4.1521, Acc: 22.79%\n",
      "  Batch 5300/5649, Loss: 3.9946, Acc: 22.80%\n",
      "  Batch 5400/5649, Loss: 4.0730, Acc: 22.80%\n",
      "  Batch 5500/5649, Loss: 4.0692, Acc: 22.80%\n",
      "  Batch 5600/5649, Loss: 4.0359, Acc: 22.80%\n",
      "Train Loss: 4.2060, Train Accuracy: 22.80%\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.1426, Acc: 22.89%\n",
      "  Batch 200/5649, Loss: 4.1452, Acc: 23.01%\n",
      "  Batch 300/5649, Loss: 4.0261, Acc: 23.09%\n",
      "  Batch 400/5649, Loss: 4.2956, Acc: 23.11%\n",
      "  Batch 500/5649, Loss: 4.0550, Acc: 23.08%\n",
      "  Batch 600/5649, Loss: 4.3368, Acc: 23.05%\n",
      "  Batch 700/5649, Loss: 4.2170, Acc: 23.01%\n",
      "  Batch 800/5649, Loss: 3.8872, Acc: 23.04%\n",
      "  Batch 900/5649, Loss: 4.1453, Acc: 22.99%\n",
      "  Batch 1000/5649, Loss: 4.2663, Acc: 23.00%\n",
      "  Batch 1100/5649, Loss: 4.2326, Acc: 22.96%\n",
      "  Batch 1200/5649, Loss: 4.2710, Acc: 22.95%\n",
      "  Batch 1300/5649, Loss: 4.1506, Acc: 22.92%\n",
      "  Batch 1300/5649, Loss: 4.1506, Acc: 22.92%\n",
      "  Batch 1400/5649, Loss: 4.1192, Acc: 22.91%\n",
      "  Batch 1400/5649, Loss: 4.1192, Acc: 22.91%\n",
      "  Batch 1500/5649, Loss: 4.1733, Acc: 22.92%\n",
      "  Batch 1500/5649, Loss: 4.1733, Acc: 22.92%\n",
      "  Batch 1600/5649, Loss: 4.2894, Acc: 22.91%\n",
      "  Batch 1600/5649, Loss: 4.2894, Acc: 22.91%\n",
      "  Batch 1700/5649, Loss: 4.2230, Acc: 22.91%\n",
      "  Batch 1700/5649, Loss: 4.2230, Acc: 22.91%\n",
      "  Batch 1800/5649, Loss: 4.0954, Acc: 22.94%\n",
      "  Batch 1800/5649, Loss: 4.0954, Acc: 22.94%\n",
      "  Batch 1900/5649, Loss: 4.2513, Acc: 22.94%\n",
      "  Batch 1900/5649, Loss: 4.2513, Acc: 22.94%\n",
      "  Batch 2000/5649, Loss: 4.2434, Acc: 22.92%\n",
      "  Batch 2000/5649, Loss: 4.2434, Acc: 22.92%\n",
      "  Batch 2100/5649, Loss: 3.9208, Acc: 22.93%\n",
      "  Batch 2100/5649, Loss: 3.9208, Acc: 22.93%\n",
      "  Batch 2200/5649, Loss: 4.1927, Acc: 22.92%\n",
      "  Batch 2200/5649, Loss: 4.1927, Acc: 22.92%\n",
      "  Batch 2300/5649, Loss: 4.0574, Acc: 22.94%\n",
      "  Batch 2300/5649, Loss: 4.0574, Acc: 22.94%\n",
      "  Batch 2400/5649, Loss: 4.2855, Acc: 22.96%\n",
      "  Batch 2400/5649, Loss: 4.2855, Acc: 22.96%\n",
      "  Batch 2500/5649, Loss: 3.9979, Acc: 22.96%\n",
      "  Batch 2500/5649, Loss: 3.9979, Acc: 22.96%\n",
      "  Batch 2600/5649, Loss: 4.0893, Acc: 22.97%\n",
      "  Batch 2600/5649, Loss: 4.0893, Acc: 22.97%\n",
      "  Batch 2700/5649, Loss: 4.1124, Acc: 22.98%\n",
      "  Batch 2700/5649, Loss: 4.1124, Acc: 22.98%\n",
      "  Batch 2800/5649, Loss: 4.2659, Acc: 22.97%\n",
      "  Batch 2800/5649, Loss: 4.2659, Acc: 22.97%\n",
      "  Batch 2900/5649, Loss: 4.1286, Acc: 22.99%\n",
      "  Batch 2900/5649, Loss: 4.1286, Acc: 22.99%\n",
      "  Batch 3000/5649, Loss: 4.2005, Acc: 22.99%\n",
      "  Batch 3000/5649, Loss: 4.2005, Acc: 22.99%\n",
      "  Batch 3100/5649, Loss: 4.3519, Acc: 23.00%\n",
      "  Batch 3100/5649, Loss: 4.3519, Acc: 23.00%\n",
      "  Batch 3200/5649, Loss: 4.2003, Acc: 23.00%\n",
      "  Batch 3200/5649, Loss: 4.2003, Acc: 23.00%\n",
      "  Batch 3300/5649, Loss: 3.9501, Acc: 23.01%\n",
      "  Batch 3300/5649, Loss: 3.9501, Acc: 23.01%\n",
      "  Batch 3400/5649, Loss: 4.0397, Acc: 23.00%\n",
      "  Batch 3400/5649, Loss: 4.0397, Acc: 23.00%\n",
      "  Batch 3500/5649, Loss: 4.0582, Acc: 23.01%\n",
      "  Batch 3500/5649, Loss: 4.0582, Acc: 23.01%\n",
      "  Batch 3600/5649, Loss: 4.2957, Acc: 23.00%\n",
      "  Batch 3600/5649, Loss: 4.2957, Acc: 23.00%\n",
      "  Batch 3700/5649, Loss: 3.8938, Acc: 23.00%\n",
      "  Batch 3700/5649, Loss: 3.8938, Acc: 23.00%\n",
      "  Batch 3800/5649, Loss: 4.0349, Acc: 23.00%\n",
      "  Batch 3800/5649, Loss: 4.0349, Acc: 23.00%\n",
      "  Batch 3900/5649, Loss: 4.1380, Acc: 23.00%\n",
      "  Batch 3900/5649, Loss: 4.1380, Acc: 23.00%\n",
      "  Batch 4000/5649, Loss: 3.9973, Acc: 23.00%\n",
      "  Batch 4000/5649, Loss: 3.9973, Acc: 23.00%\n",
      "  Batch 4100/5649, Loss: 4.0590, Acc: 23.00%\n",
      "  Batch 4100/5649, Loss: 4.0590, Acc: 23.00%\n",
      "  Batch 4200/5649, Loss: 4.3715, Acc: 23.01%\n",
      "  Batch 4200/5649, Loss: 4.3715, Acc: 23.01%\n",
      "  Batch 4300/5649, Loss: 4.2735, Acc: 23.01%\n",
      "  Batch 4300/5649, Loss: 4.2735, Acc: 23.01%\n",
      "  Batch 4400/5649, Loss: 4.2534, Acc: 23.02%\n",
      "  Batch 4400/5649, Loss: 4.2534, Acc: 23.02%\n",
      "  Batch 4500/5649, Loss: 4.1918, Acc: 23.01%\n",
      "  Batch 4500/5649, Loss: 4.1918, Acc: 23.01%\n",
      "  Batch 4600/5649, Loss: 4.1399, Acc: 23.02%\n",
      "  Batch 4600/5649, Loss: 4.1399, Acc: 23.02%\n",
      "  Batch 4700/5649, Loss: 4.0593, Acc: 23.02%\n",
      "  Batch 4700/5649, Loss: 4.0593, Acc: 23.02%\n",
      "  Batch 4800/5649, Loss: 4.3713, Acc: 23.02%\n",
      "  Batch 4800/5649, Loss: 4.3713, Acc: 23.02%\n",
      "  Batch 4900/5649, Loss: 4.4290, Acc: 23.02%\n",
      "  Batch 4900/5649, Loss: 4.4290, Acc: 23.02%\n",
      "  Batch 5000/5649, Loss: 4.2833, Acc: 23.02%\n",
      "  Batch 5000/5649, Loss: 4.2833, Acc: 23.02%\n",
      "  Batch 5100/5649, Loss: 3.9637, Acc: 23.02%\n",
      "  Batch 5100/5649, Loss: 3.9637, Acc: 23.02%\n",
      "  Batch 5200/5649, Loss: 4.0431, Acc: 23.03%\n",
      "  Batch 5200/5649, Loss: 4.0431, Acc: 23.03%\n",
      "  Batch 5300/5649, Loss: 4.1173, Acc: 23.03%\n",
      "  Batch 5300/5649, Loss: 4.1173, Acc: 23.03%\n",
      "  Batch 5400/5649, Loss: 4.2877, Acc: 23.03%\n",
      "  Batch 5400/5649, Loss: 4.2877, Acc: 23.03%\n",
      "  Batch 5500/5649, Loss: 4.1140, Acc: 23.03%\n",
      "  Batch 5500/5649, Loss: 4.1140, Acc: 23.03%\n",
      "  Batch 5600/5649, Loss: 4.1344, Acc: 23.03%\n",
      "  Batch 5600/5649, Loss: 4.1344, Acc: 23.03%\n",
      "Train Loss: 4.1658, Train Accuracy: 23.03%\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------------------------------------\n",
      "Train Loss: 4.1658, Train Accuracy: 23.03%\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 3.9759, Acc: 23.04%\n",
      "  Batch 100/5649, Loss: 3.9759, Acc: 23.04%\n",
      "  Batch 200/5649, Loss: 4.2117, Acc: 23.07%\n",
      "  Batch 200/5649, Loss: 4.2117, Acc: 23.07%\n",
      "  Batch 300/5649, Loss: 4.1510, Acc: 23.07%\n",
      "  Batch 300/5649, Loss: 4.1510, Acc: 23.07%\n",
      "  Batch 400/5649, Loss: 4.0265, Acc: 23.05%\n",
      "  Batch 400/5649, Loss: 4.0265, Acc: 23.05%\n",
      "  Batch 500/5649, Loss: 4.1717, Acc: 23.11%\n",
      "  Batch 500/5649, Loss: 4.1717, Acc: 23.11%\n",
      "  Batch 600/5649, Loss: 3.9754, Acc: 23.11%\n",
      "  Batch 600/5649, Loss: 3.9754, Acc: 23.11%\n",
      "  Batch 700/5649, Loss: 4.0437, Acc: 23.15%\n",
      "  Batch 700/5649, Loss: 4.0437, Acc: 23.15%\n",
      "  Batch 800/5649, Loss: 4.0909, Acc: 23.15%\n",
      "  Batch 800/5649, Loss: 4.0909, Acc: 23.15%\n",
      "  Batch 900/5649, Loss: 4.0339, Acc: 23.16%\n",
      "  Batch 900/5649, Loss: 4.0339, Acc: 23.16%\n",
      "  Batch 1000/5649, Loss: 4.0851, Acc: 23.16%\n",
      "  Batch 1000/5649, Loss: 4.0851, Acc: 23.16%\n",
      "  Batch 1100/5649, Loss: 4.1496, Acc: 23.14%\n",
      "  Batch 1100/5649, Loss: 4.1496, Acc: 23.14%\n",
      "  Batch 1200/5649, Loss: 3.9704, Acc: 23.15%\n",
      "  Batch 1200/5649, Loss: 3.9704, Acc: 23.15%\n",
      "  Batch 1300/5649, Loss: 3.9357, Acc: 23.15%\n",
      "  Batch 1300/5649, Loss: 3.9357, Acc: 23.15%\n",
      "  Batch 1400/5649, Loss: 4.1540, Acc: 23.13%\n",
      "  Batch 1400/5649, Loss: 4.1540, Acc: 23.13%\n",
      "  Batch 1500/5649, Loss: 4.0407, Acc: 23.14%\n",
      "  Batch 1500/5649, Loss: 4.0407, Acc: 23.14%\n",
      "  Batch 1600/5649, Loss: 4.2021, Acc: 23.14%\n",
      "  Batch 1600/5649, Loss: 4.2021, Acc: 23.14%\n",
      "  Batch 1700/5649, Loss: 4.1826, Acc: 23.15%\n",
      "  Batch 1700/5649, Loss: 4.1826, Acc: 23.15%\n",
      "  Batch 1800/5649, Loss: 4.1642, Acc: 23.14%\n",
      "  Batch 1800/5649, Loss: 4.1642, Acc: 23.14%\n",
      "  Batch 1900/5649, Loss: 4.1384, Acc: 23.15%\n",
      "  Batch 1900/5649, Loss: 4.1384, Acc: 23.15%\n",
      "  Batch 2000/5649, Loss: 4.0600, Acc: 23.15%\n",
      "  Batch 2100/5649, Loss: 4.2971, Acc: 23.14%\n",
      "  Batch 2200/5649, Loss: 4.1313, Acc: 23.14%\n",
      "  Batch 2300/5649, Loss: 4.0632, Acc: 23.15%\n",
      "  Batch 2400/5649, Loss: 3.9632, Acc: 23.15%\n",
      "  Batch 2500/5649, Loss: 4.2719, Acc: 23.16%\n",
      "  Batch 2600/5649, Loss: 3.8604, Acc: 23.16%\n",
      "  Batch 2700/5649, Loss: 4.3012, Acc: 23.17%\n",
      "  Batch 2800/5649, Loss: 4.1497, Acc: 23.16%\n",
      "  Batch 2900/5649, Loss: 4.1475, Acc: 23.16%\n",
      "  Batch 3000/5649, Loss: 4.1256, Acc: 23.16%\n",
      "  Batch 3100/5649, Loss: 4.1758, Acc: 23.14%\n",
      "  Batch 3200/5649, Loss: 4.1446, Acc: 23.15%\n",
      "  Batch 3300/5649, Loss: 3.9927, Acc: 23.16%\n",
      "  Batch 3400/5649, Loss: 4.2145, Acc: 23.16%\n",
      "  Batch 3500/5649, Loss: 4.0478, Acc: 23.15%\n",
      "  Batch 3600/5649, Loss: 4.2184, Acc: 23.15%\n",
      "  Batch 3700/5649, Loss: 4.3580, Acc: 23.15%\n",
      "  Batch 3800/5649, Loss: 4.0043, Acc: 23.16%\n",
      "  Batch 3900/5649, Loss: 4.1457, Acc: 23.16%\n",
      "  Batch 4000/5649, Loss: 4.0147, Acc: 23.16%\n",
      "  Batch 4100/5649, Loss: 4.3464, Acc: 23.16%\n",
      "  Batch 4200/5649, Loss: 4.0173, Acc: 23.16%\n",
      "  Batch 4300/5649, Loss: 4.1197, Acc: 23.16%\n",
      "  Batch 4400/5649, Loss: 4.3904, Acc: 23.16%\n",
      "  Batch 4500/5649, Loss: 4.0682, Acc: 23.15%\n",
      "  Batch 4600/5649, Loss: 4.1977, Acc: 23.15%\n",
      "  Batch 4700/5649, Loss: 4.3244, Acc: 23.15%\n",
      "  Batch 4800/5649, Loss: 4.0218, Acc: 23.16%\n",
      "  Batch 4900/5649, Loss: 4.1711, Acc: 23.16%\n",
      "  Batch 5000/5649, Loss: 4.2558, Acc: 23.16%\n",
      "  Batch 5100/5649, Loss: 4.1892, Acc: 23.16%\n",
      "  Batch 5200/5649, Loss: 3.9552, Acc: 23.16%\n",
      "  Batch 5300/5649, Loss: 4.1152, Acc: 23.17%\n",
      "  Batch 5400/5649, Loss: 4.2236, Acc: 23.17%\n",
      "  Batch 5500/5649, Loss: 3.9885, Acc: 23.17%\n",
      "  Batch 5600/5649, Loss: 4.2065, Acc: 23.17%\n",
      "Train Loss: 4.1400, Train Accuracy: 23.17%\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.2937, Acc: 23.58%\n",
      "  Batch 200/5649, Loss: 4.1468, Acc: 23.62%\n",
      "  Batch 300/5649, Loss: 4.2611, Acc: 23.45%\n",
      "  Batch 400/5649, Loss: 4.0000, Acc: 23.42%\n",
      "  Batch 500/5649, Loss: 4.2398, Acc: 23.36%\n",
      "  Batch 600/5649, Loss: 3.9285, Acc: 23.37%\n",
      "  Batch 700/5649, Loss: 3.9617, Acc: 23.32%\n",
      "  Batch 800/5649, Loss: 4.1343, Acc: 23.31%\n",
      "  Batch 900/5649, Loss: 4.0335, Acc: 23.35%\n",
      "  Batch 1000/5649, Loss: 4.3620, Acc: 23.32%\n",
      "  Batch 1100/5649, Loss: 4.1167, Acc: 23.31%\n",
      "  Batch 1200/5649, Loss: 4.3469, Acc: 23.31%\n",
      "  Batch 1300/5649, Loss: 3.8630, Acc: 23.29%\n",
      "  Batch 1400/5649, Loss: 4.0518, Acc: 23.28%\n",
      "  Batch 1500/5649, Loss: 4.0559, Acc: 23.29%\n",
      "  Batch 1600/5649, Loss: 3.9677, Acc: 23.30%\n",
      "  Batch 1700/5649, Loss: 4.0062, Acc: 23.28%\n",
      "  Batch 1800/5649, Loss: 3.8883, Acc: 23.30%\n",
      "  Batch 1900/5649, Loss: 4.1334, Acc: 23.29%\n",
      "  Batch 2000/5649, Loss: 4.1565, Acc: 23.27%\n",
      "  Batch 2100/5649, Loss: 4.1959, Acc: 23.29%\n",
      "  Batch 2200/5649, Loss: 4.1711, Acc: 23.30%\n",
      "  Batch 2300/5649, Loss: 4.0470, Acc: 23.31%\n",
      "  Batch 2400/5649, Loss: 4.1325, Acc: 23.31%\n",
      "  Batch 2500/5649, Loss: 4.3489, Acc: 23.31%\n",
      "  Batch 2600/5649, Loss: 3.7620, Acc: 23.31%\n",
      "  Batch 2700/5649, Loss: 4.1722, Acc: 23.32%\n",
      "  Batch 2800/5649, Loss: 3.8701, Acc: 23.32%\n",
      "  Batch 2900/5649, Loss: 4.0412, Acc: 23.31%\n",
      "  Batch 3000/5649, Loss: 4.2143, Acc: 23.30%\n",
      "  Batch 3100/5649, Loss: 4.0397, Acc: 23.31%\n",
      "  Batch 3200/5649, Loss: 4.3512, Acc: 23.31%\n",
      "  Batch 3300/5649, Loss: 4.2490, Acc: 23.30%\n",
      "  Batch 3400/5649, Loss: 4.1032, Acc: 23.31%\n",
      "  Batch 3500/5649, Loss: 4.1055, Acc: 23.32%\n",
      "  Batch 3600/5649, Loss: 4.2895, Acc: 23.31%\n",
      "  Batch 3700/5649, Loss: 4.4041, Acc: 23.32%\n",
      "  Batch 3800/5649, Loss: 4.2120, Acc: 23.31%\n",
      "  Batch 3900/5649, Loss: 3.8443, Acc: 23.31%\n",
      "  Batch 4000/5649, Loss: 4.1430, Acc: 23.32%\n",
      "  Batch 4100/5649, Loss: 4.0870, Acc: 23.32%\n",
      "  Batch 4200/5649, Loss: 3.9400, Acc: 23.32%\n",
      "  Batch 4300/5649, Loss: 3.8967, Acc: 23.31%\n",
      "  Batch 4400/5649, Loss: 4.2545, Acc: 23.32%\n",
      "  Batch 4500/5649, Loss: 4.1858, Acc: 23.31%\n",
      "  Batch 4600/5649, Loss: 4.2669, Acc: 23.32%\n",
      "  Batch 4700/5649, Loss: 4.3329, Acc: 23.32%\n",
      "  Batch 4800/5649, Loss: 3.9040, Acc: 23.31%\n",
      "  Batch 4900/5649, Loss: 4.1526, Acc: 23.31%\n",
      "  Batch 5000/5649, Loss: 4.0130, Acc: 23.31%\n",
      "  Batch 5100/5649, Loss: 3.9373, Acc: 23.31%\n",
      "  Batch 5200/5649, Loss: 4.1944, Acc: 23.32%\n",
      "  Batch 5300/5649, Loss: 3.9507, Acc: 23.33%\n",
      "  Batch 5400/5649, Loss: 4.1583, Acc: 23.33%\n",
      "  Batch 5500/5649, Loss: 4.1146, Acc: 23.33%\n",
      "  Batch 5600/5649, Loss: 3.9340, Acc: 23.33%\n",
      "Train Loss: 4.1212, Train Accuracy: 23.33%\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.2890, Acc: 23.43%\n",
      "  Batch 200/5649, Loss: 4.1469, Acc: 23.30%\n",
      "  Batch 300/5649, Loss: 4.0918, Acc: 23.33%\n",
      "  Batch 400/5649, Loss: 3.9691, Acc: 23.36%\n",
      "  Batch 500/5649, Loss: 4.3448, Acc: 23.43%\n",
      "  Batch 600/5649, Loss: 4.3090, Acc: 23.40%\n",
      "  Batch 700/5649, Loss: 4.1484, Acc: 23.39%\n",
      "  Batch 800/5649, Loss: 3.9895, Acc: 23.38%\n",
      "  Batch 900/5649, Loss: 4.2854, Acc: 23.38%\n",
      "  Batch 1000/5649, Loss: 4.1873, Acc: 23.36%\n",
      "  Batch 1100/5649, Loss: 4.3250, Acc: 23.35%\n",
      "  Batch 1200/5649, Loss: 4.0750, Acc: 23.37%\n",
      "  Batch 1300/5649, Loss: 3.9484, Acc: 23.37%\n",
      "  Batch 1400/5649, Loss: 4.0784, Acc: 23.38%\n",
      "  Batch 1500/5649, Loss: 4.0269, Acc: 23.35%\n",
      "  Batch 1600/5649, Loss: 4.1893, Acc: 23.35%\n",
      "  Batch 1700/5649, Loss: 3.9586, Acc: 23.34%\n",
      "  Batch 1800/5649, Loss: 4.1426, Acc: 23.36%\n",
      "  Batch 1900/5649, Loss: 3.9655, Acc: 23.38%\n",
      "  Batch 2000/5649, Loss: 4.2433, Acc: 23.39%\n",
      "  Batch 2100/5649, Loss: 4.3697, Acc: 23.39%\n",
      "  Batch 2200/5649, Loss: 3.9275, Acc: 23.40%\n",
      "  Batch 2300/5649, Loss: 3.9748, Acc: 23.39%\n",
      "  Batch 2400/5649, Loss: 4.1054, Acc: 23.40%\n",
      "  Batch 2500/5649, Loss: 4.0108, Acc: 23.40%\n",
      "  Batch 2600/5649, Loss: 3.9507, Acc: 23.39%\n",
      "  Batch 2700/5649, Loss: 3.9028, Acc: 23.39%\n",
      "  Batch 2800/5649, Loss: 3.9734, Acc: 23.39%\n",
      "  Batch 2900/5649, Loss: 4.0655, Acc: 23.38%\n",
      "  Batch 3000/5649, Loss: 3.9371, Acc: 23.39%\n",
      "  Batch 3100/5649, Loss: 4.0934, Acc: 23.39%\n",
      "  Batch 3200/5649, Loss: 4.1071, Acc: 23.39%\n",
      "  Batch 3300/5649, Loss: 4.1892, Acc: 23.40%\n",
      "  Batch 3400/5649, Loss: 4.1702, Acc: 23.39%\n",
      "  Batch 3500/5649, Loss: 4.3027, Acc: 23.39%\n",
      "  Batch 3600/5649, Loss: 3.9830, Acc: 23.38%\n",
      "  Batch 3700/5649, Loss: 4.1201, Acc: 23.39%\n",
      "  Batch 3800/5649, Loss: 4.0810, Acc: 23.38%\n",
      "  Batch 3900/5649, Loss: 4.2530, Acc: 23.38%\n",
      "  Batch 4000/5649, Loss: 4.0833, Acc: 23.37%\n",
      "  Batch 4100/5649, Loss: 4.4277, Acc: 23.38%\n",
      "  Batch 4200/5649, Loss: 4.1214, Acc: 23.38%\n",
      "  Batch 4300/5649, Loss: 4.0989, Acc: 23.37%\n",
      "  Batch 4400/5649, Loss: 3.8916, Acc: 23.37%\n",
      "  Batch 4500/5649, Loss: 3.9128, Acc: 23.37%\n",
      "  Batch 4600/5649, Loss: 4.0662, Acc: 23.37%\n",
      "  Batch 4700/5649, Loss: 3.9727, Acc: 23.37%\n",
      "  Batch 4800/5649, Loss: 4.1744, Acc: 23.38%\n",
      "  Batch 4900/5649, Loss: 3.9192, Acc: 23.38%\n",
      "  Batch 5000/5649, Loss: 4.0221, Acc: 23.38%\n",
      "  Batch 5100/5649, Loss: 4.0130, Acc: 23.37%\n",
      "  Batch 5200/5649, Loss: 3.9749, Acc: 23.38%\n",
      "  Batch 5300/5649, Loss: 4.4914, Acc: 23.38%\n",
      "  Batch 5400/5649, Loss: 4.0793, Acc: 23.38%\n",
      "  Batch 5500/5649, Loss: 4.0350, Acc: 23.38%\n",
      "  Batch 5600/5649, Loss: 4.1641, Acc: 23.38%\n",
      "Train Loss: 4.1071, Train Accuracy: 23.38%\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.0047, Acc: 23.14%\n",
      "  Batch 200/5649, Loss: 3.9107, Acc: 23.34%\n",
      "  Batch 300/5649, Loss: 3.9599, Acc: 23.40%\n",
      "  Batch 400/5649, Loss: 4.2133, Acc: 23.41%\n",
      "  Batch 500/5649, Loss: 4.0846, Acc: 23.43%\n",
      "  Batch 600/5649, Loss: 4.1370, Acc: 23.48%\n",
      "  Batch 700/5649, Loss: 4.0525, Acc: 23.48%\n",
      "  Batch 800/5649, Loss: 4.1230, Acc: 23.48%\n",
      "  Batch 900/5649, Loss: 3.8303, Acc: 23.48%\n",
      "  Batch 1000/5649, Loss: 3.9126, Acc: 23.50%\n",
      "  Batch 1100/5649, Loss: 4.1357, Acc: 23.49%\n",
      "  Batch 1200/5649, Loss: 4.0249, Acc: 23.48%\n",
      "  Batch 1300/5649, Loss: 3.9679, Acc: 23.45%\n",
      "  Batch 1400/5649, Loss: 4.1670, Acc: 23.46%\n",
      "  Batch 1500/5649, Loss: 4.1319, Acc: 23.48%\n",
      "  Batch 1600/5649, Loss: 4.1524, Acc: 23.48%\n",
      "  Batch 1700/5649, Loss: 4.1149, Acc: 23.49%\n",
      "  Batch 1800/5649, Loss: 4.2763, Acc: 23.48%\n",
      "  Batch 1900/5649, Loss: 4.1485, Acc: 23.49%\n",
      "  Batch 2000/5649, Loss: 4.1844, Acc: 23.50%\n",
      "  Batch 2100/5649, Loss: 4.0940, Acc: 23.50%\n",
      "  Batch 2200/5649, Loss: 4.1068, Acc: 23.49%\n",
      "  Batch 2300/5649, Loss: 4.0494, Acc: 23.48%\n",
      "  Batch 2400/5649, Loss: 3.9474, Acc: 23.48%\n",
      "  Batch 2500/5649, Loss: 4.2372, Acc: 23.48%\n",
      "  Batch 2600/5649, Loss: 4.2549, Acc: 23.49%\n",
      "  Batch 2700/5649, Loss: 3.8466, Acc: 23.49%\n",
      "  Batch 2800/5649, Loss: 4.1424, Acc: 23.49%\n",
      "  Batch 2900/5649, Loss: 3.9797, Acc: 23.49%\n",
      "  Batch 3000/5649, Loss: 4.1665, Acc: 23.49%\n",
      "  Batch 3100/5649, Loss: 4.0723, Acc: 23.48%\n",
      "  Batch 3200/5649, Loss: 4.0039, Acc: 23.47%\n",
      "  Batch 3300/5649, Loss: 4.0782, Acc: 23.46%\n",
      "  Batch 3400/5649, Loss: 3.9408, Acc: 23.47%\n",
      "  Batch 3500/5649, Loss: 4.2187, Acc: 23.46%\n",
      "  Batch 3600/5649, Loss: 3.9437, Acc: 23.46%\n",
      "  Batch 3700/5649, Loss: 4.3034, Acc: 23.45%\n",
      "  Batch 3800/5649, Loss: 4.0511, Acc: 23.44%\n",
      "  Batch 3900/5649, Loss: 3.9744, Acc: 23.43%\n",
      "  Batch 4000/5649, Loss: 4.1423, Acc: 23.43%\n",
      "  Batch 4100/5649, Loss: 4.2955, Acc: 23.43%\n",
      "  Batch 4200/5649, Loss: 4.2325, Acc: 23.43%\n",
      "  Batch 4300/5649, Loss: 4.0585, Acc: 23.43%\n",
      "  Batch 4400/5649, Loss: 4.1152, Acc: 23.43%\n",
      "  Batch 4500/5649, Loss: 4.0577, Acc: 23.44%\n",
      "  Batch 4600/5649, Loss: 4.1644, Acc: 23.44%\n",
      "  Batch 4700/5649, Loss: 3.9769, Acc: 23.44%\n",
      "  Batch 4800/5649, Loss: 4.1343, Acc: 23.44%\n",
      "  Batch 4900/5649, Loss: 4.2295, Acc: 23.44%\n",
      "  Batch 5000/5649, Loss: 4.1055, Acc: 23.43%\n",
      "  Batch 5100/5649, Loss: 4.2639, Acc: 23.43%\n",
      "  Batch 5200/5649, Loss: 4.4308, Acc: 23.43%\n",
      "  Batch 5300/5649, Loss: 3.8504, Acc: 23.43%\n",
      "  Batch 5400/5649, Loss: 4.0519, Acc: 23.44%\n",
      "  Batch 5500/5649, Loss: 4.1482, Acc: 23.45%\n",
      "  Batch 5600/5649, Loss: 3.9149, Acc: 23.44%\n",
      "Train Loss: 4.0972, Train Accuracy: 23.44%\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.2527, Acc: 23.54%\n",
      "  Batch 200/5649, Loss: 4.2006, Acc: 23.67%\n",
      "  Batch 300/5649, Loss: 4.0708, Acc: 23.47%\n",
      "  Batch 400/5649, Loss: 4.0756, Acc: 23.57%\n",
      "  Batch 500/5649, Loss: 4.2807, Acc: 23.51%\n",
      "  Batch 600/5649, Loss: 4.1762, Acc: 23.51%\n",
      "  Batch 700/5649, Loss: 4.1540, Acc: 23.51%\n",
      "  Batch 800/5649, Loss: 4.0095, Acc: 23.51%\n",
      "  Batch 900/5649, Loss: 4.2636, Acc: 23.50%\n",
      "  Batch 1000/5649, Loss: 4.1370, Acc: 23.47%\n",
      "  Batch 1100/5649, Loss: 3.9704, Acc: 23.50%\n",
      "  Batch 1200/5649, Loss: 3.9579, Acc: 23.49%\n",
      "  Batch 1300/5649, Loss: 3.9258, Acc: 23.47%\n",
      "  Batch 1400/5649, Loss: 4.1295, Acc: 23.48%\n",
      "  Batch 1500/5649, Loss: 4.1025, Acc: 23.49%\n",
      "  Batch 1600/5649, Loss: 4.1996, Acc: 23.47%\n",
      "  Batch 1700/5649, Loss: 4.1574, Acc: 23.47%\n",
      "  Batch 1800/5649, Loss: 4.0985, Acc: 23.47%\n",
      "  Batch 1900/5649, Loss: 4.2030, Acc: 23.47%\n",
      "  Batch 2000/5649, Loss: 3.9512, Acc: 23.49%\n",
      "  Batch 2100/5649, Loss: 4.1819, Acc: 23.48%\n",
      "  Batch 2200/5649, Loss: 4.2838, Acc: 23.49%\n",
      "  Batch 2300/5649, Loss: 4.0713, Acc: 23.50%\n",
      "  Batch 2400/5649, Loss: 4.0433, Acc: 23.48%\n",
      "  Batch 2500/5649, Loss: 4.3349, Acc: 23.47%\n",
      "  Batch 2600/5649, Loss: 3.9217, Acc: 23.47%\n",
      "  Batch 2700/5649, Loss: 3.9111, Acc: 23.47%\n",
      "  Batch 2800/5649, Loss: 4.2455, Acc: 23.46%\n",
      "  Batch 2900/5649, Loss: 4.1859, Acc: 23.45%\n",
      "  Batch 3000/5649, Loss: 4.0755, Acc: 23.44%\n",
      "  Batch 3100/5649, Loss: 4.1484, Acc: 23.44%\n",
      "  Batch 3200/5649, Loss: 4.1860, Acc: 23.45%\n",
      "  Batch 3300/5649, Loss: 4.0591, Acc: 23.45%\n",
      "  Batch 3400/5649, Loss: 4.1027, Acc: 23.46%\n",
      "  Batch 3500/5649, Loss: 4.1840, Acc: 23.47%\n",
      "  Batch 3600/5649, Loss: 3.9921, Acc: 23.47%\n",
      "  Batch 3700/5649, Loss: 4.0004, Acc: 23.47%\n",
      "  Batch 3800/5649, Loss: 4.0701, Acc: 23.48%\n",
      "  Batch 3900/5649, Loss: 4.1447, Acc: 23.49%\n",
      "  Batch 4000/5649, Loss: 4.0286, Acc: 23.48%\n",
      "  Batch 4100/5649, Loss: 4.2073, Acc: 23.47%\n",
      "  Batch 4200/5649, Loss: 4.0635, Acc: 23.47%\n",
      "  Batch 4300/5649, Loss: 3.9945, Acc: 23.48%\n",
      "  Batch 4400/5649, Loss: 4.1165, Acc: 23.48%\n",
      "  Batch 4500/5649, Loss: 3.9800, Acc: 23.48%\n",
      "  Batch 4600/5649, Loss: 4.2813, Acc: 23.48%\n",
      "  Batch 4700/5649, Loss: 4.0942, Acc: 23.48%\n",
      "  Batch 4800/5649, Loss: 4.2460, Acc: 23.48%\n",
      "  Batch 4900/5649, Loss: 4.0460, Acc: 23.48%\n",
      "  Batch 5000/5649, Loss: 4.0468, Acc: 23.48%\n",
      "  Batch 5100/5649, Loss: 4.1032, Acc: 23.47%\n",
      "  Batch 5200/5649, Loss: 4.1478, Acc: 23.47%\n",
      "  Batch 5300/5649, Loss: 3.9280, Acc: 23.46%\n",
      "  Batch 5400/5649, Loss: 4.1663, Acc: 23.46%\n",
      "  Batch 5500/5649, Loss: 4.0739, Acc: 23.46%\n",
      "  Batch 5600/5649, Loss: 3.9998, Acc: 23.46%\n",
      "Train Loss: 4.0895, Train Accuracy: 23.46%\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.3509, Acc: 23.78%\n",
      "  Batch 200/5649, Loss: 4.2360, Acc: 23.58%\n",
      "  Batch 300/5649, Loss: 4.1651, Acc: 23.47%\n",
      "  Batch 400/5649, Loss: 3.9707, Acc: 23.41%\n",
      "  Batch 500/5649, Loss: 4.0615, Acc: 23.41%\n",
      "  Batch 600/5649, Loss: 3.9021, Acc: 23.35%\n",
      "  Batch 700/5649, Loss: 4.1658, Acc: 23.38%\n",
      "  Batch 800/5649, Loss: 4.0704, Acc: 23.39%\n",
      "  Batch 900/5649, Loss: 4.0921, Acc: 23.42%\n",
      "  Batch 1000/5649, Loss: 4.0612, Acc: 23.43%\n",
      "  Batch 1100/5649, Loss: 3.9320, Acc: 23.44%\n",
      "  Batch 1200/5649, Loss: 4.0727, Acc: 23.46%\n",
      "  Batch 1300/5649, Loss: 3.9380, Acc: 23.45%\n",
      "  Batch 1400/5649, Loss: 3.8487, Acc: 23.45%\n",
      "  Batch 1500/5649, Loss: 4.0777, Acc: 23.46%\n",
      "  Batch 1600/5649, Loss: 4.1134, Acc: 23.47%\n",
      "  Batch 1700/5649, Loss: 4.0511, Acc: 23.46%\n",
      "  Batch 1800/5649, Loss: 3.9791, Acc: 23.46%\n",
      "  Batch 1900/5649, Loss: 3.8098, Acc: 23.45%\n",
      "  Batch 2000/5649, Loss: 4.0887, Acc: 23.44%\n",
      "  Batch 2100/5649, Loss: 4.2088, Acc: 23.43%\n",
      "  Batch 2200/5649, Loss: 4.2186, Acc: 23.44%\n",
      "  Batch 2300/5649, Loss: 3.9808, Acc: 23.44%\n",
      "  Batch 2400/5649, Loss: 4.2860, Acc: 23.44%\n",
      "  Batch 2500/5649, Loss: 4.0318, Acc: 23.44%\n",
      "  Batch 2600/5649, Loss: 3.9919, Acc: 23.44%\n",
      "  Batch 2700/5649, Loss: 4.2689, Acc: 23.44%\n",
      "  Batch 2800/5649, Loss: 3.7349, Acc: 23.45%\n",
      "  Batch 2900/5649, Loss: 4.0393, Acc: 23.46%\n",
      "  Batch 3000/5649, Loss: 3.9117, Acc: 23.45%\n",
      "  Batch 3100/5649, Loss: 4.0095, Acc: 23.46%\n",
      "  Batch 3200/5649, Loss: 4.1489, Acc: 23.47%\n",
      "  Batch 3300/5649, Loss: 4.1220, Acc: 23.46%\n",
      "  Batch 3400/5649, Loss: 4.2268, Acc: 23.45%\n",
      "  Batch 3500/5649, Loss: 4.0019, Acc: 23.46%\n",
      "  Batch 3600/5649, Loss: 4.0498, Acc: 23.46%\n",
      "  Batch 3700/5649, Loss: 4.2400, Acc: 23.46%\n",
      "  Batch 3800/5649, Loss: 3.9611, Acc: 23.46%\n",
      "  Batch 3900/5649, Loss: 4.0238, Acc: 23.46%\n",
      "  Batch 4000/5649, Loss: 4.3868, Acc: 23.46%\n",
      "  Batch 4100/5649, Loss: 4.0933, Acc: 23.47%\n",
      "  Batch 4200/5649, Loss: 4.1256, Acc: 23.47%\n",
      "  Batch 4300/5649, Loss: 3.8332, Acc: 23.47%\n",
      "  Batch 4400/5649, Loss: 4.1054, Acc: 23.48%\n",
      "  Batch 4500/5649, Loss: 4.5211, Acc: 23.48%\n",
      "  Batch 4600/5649, Loss: 4.0047, Acc: 23.48%\n",
      "  Batch 4700/5649, Loss: 4.0567, Acc: 23.48%\n",
      "  Batch 4800/5649, Loss: 4.2147, Acc: 23.48%\n",
      "  Batch 4900/5649, Loss: 4.2101, Acc: 23.48%\n",
      "  Batch 5000/5649, Loss: 3.9911, Acc: 23.48%\n",
      "  Batch 5100/5649, Loss: 4.0155, Acc: 23.47%\n",
      "  Batch 5200/5649, Loss: 4.2035, Acc: 23.47%\n",
      "  Batch 5300/5649, Loss: 4.1377, Acc: 23.48%\n",
      "  Batch 5400/5649, Loss: 4.0362, Acc: 23.48%\n",
      "  Batch 5500/5649, Loss: 4.2187, Acc: 23.48%\n",
      "  Batch 5600/5649, Loss: 3.9071, Acc: 23.49%\n",
      "Train Loss: 4.0821, Train Accuracy: 23.49%\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "# Store training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING RNN LANGUAGE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_rnn(rnn_model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe81bc",
   "metadata": {},
   "source": [
    "## Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7290f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Accuracy\n",
    "axes[1].plot(epochs, [acc * 100 for acc in history['train_acc']], 'g-o', label='Training Accuracy', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cbc759",
   "metadata": {},
   "source": [
    "## Evaluate the RNN model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91686622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 23.00%, Perplexity: 86.50\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test_loss, test_acc, test_perplexity = evaluate_rnn(rnn_model, test_loader, criterion, device)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%, Perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bcee1",
   "metadata": {},
   "source": [
    "## Text Generation with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce82f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RNN TEXT GENERATION\n",
      "================================================================================\n",
      "\n",
      "Sentence 0:\n",
      "  Incomplete: cover with\n",
      "  Completed:  cover with plastic wrap\n",
      "  Length: 4 words\n",
      "\n",
      "Sentence 1:\n",
      "  Incomplete: roll up\n",
      "  Completed:  roll up with the petals for the bottom of chicken is silky and transfer the nuts 1 / 2 inch\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 2:\n",
      "  Incomplete: cook the\n",
      "  Completed:  cover with plastic wrap\n",
      "  Length: 4 words\n",
      "\n",
      "Sentence 1:\n",
      "  Incomplete: roll up\n",
      "  Completed:  roll up with the petals for the bottom of chicken is silky and transfer the nuts 1 / 2 inch\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 2:\n",
      "  Incomplete: cook the\n",
      "  Completed:  cook the bottom of the top of the lower the icing over a wooden spoon to loosen the oven for\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 3:\n",
      "  Incomplete: stir in\n",
      "  Completed:  stir in the basil leaves the cheese and place on how much as i thicken and brown and place on\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 4:\n",
      "  Incomplete: spread out\n",
      "  Completed:  spread out as you want to blend together with the whipped cream for the mixture in the onion and put\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 5:\n",
      "  Incomplete: transfer the\n",
      "  Completed:  transfer the oven for 20 minutes or until it and then lay them for 30-40 minutes until golden brown and\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 6:\n",
      "  Incomplete: put the\n",
      "  Completed:  put the bottom of a small bowl or get the pasta is ready when you begin to oven and blitz\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 7:\n",
      "  Incomplete: push the\n",
      "  Completed:  push the first to make the raisins or according to remove the way up to cool to use and then\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 8:\n",
      "  Incomplete: cut into\n",
      "  Completed:  cut into the bottom on each popover to 350f when the freezer 30 minutes or until the soup bowls and\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 9:\n",
      "  Incomplete: toss the\n",
      "  Completed:  cook the bottom of the top of the lower the icing over a wooden spoon to loosen the oven for\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 3:\n",
      "  Incomplete: stir in\n",
      "  Completed:  stir in the basil leaves the cheese and place on how much as i thicken and brown and place on\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 4:\n",
      "  Incomplete: spread out\n",
      "  Completed:  spread out as you want to blend together with the whipped cream for the mixture in the onion and put\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 5:\n",
      "  Incomplete: transfer the\n",
      "  Completed:  transfer the oven for 20 minutes or until it and then lay them for 30-40 minutes until golden brown and\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 6:\n",
      "  Incomplete: put the\n",
      "  Completed:  put the bottom of a small bowl or get the pasta is ready when you begin to oven and blitz\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 7:\n",
      "  Incomplete: push the\n",
      "  Completed:  push the first to make the raisins or according to remove the way up to cool to use and then\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 8:\n",
      "  Incomplete: cut into\n",
      "  Completed:  cut into the bottom on each popover to 350f when the freezer 30 minutes or until the soup bowls and\n",
      "  Length: 20 words\n",
      "\n",
      "Sentence 9:\n",
      "  Incomplete: toss the\n",
      "  Completed:  toss the surface and wrap and then add the sides of the microwave for about 2 cup honey then continue\n",
      "  Length: 20 words\n",
      "  Completed:  toss the surface and wrap and then add the sides of the microwave for about 2 cup honey then continue\n",
      "  Length: 20 words\n"
     ]
    }
   ],
   "source": [
    "def generate_text_rnn(model, start_words, vocab, max_length=20, temperature=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using the trained RNN model\n",
    "    Args:\n",
    "        model: Trained RNN model\n",
    "        start_words: List of starting words\n",
    "        vocab: Vocabulary dictionary\n",
    "        max_length: Maximum length of generated sequence\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        device: Device to run on\n",
    "    Returns:\n",
    "        Generated text as a list of words\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create reverse vocabulary\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    # Convert start words to indices\n",
    "    current_seq = [vocab.get(word, vocab['<unk>']) for word in start_words]\n",
    "    generated = list(start_words)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_words)):\n",
    "            # Prepare input\n",
    "            input_tensor = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Get model prediction\n",
    "            output, _ = model(input_tensor)\n",
    "            \n",
    "            # Get last word prediction\n",
    "            logits = output[0, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "            \n",
    "            # Sample next word\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            next_word = idx_to_word.get(next_idx, '<unk>')\n",
    "            \n",
    "            # Stop if end of sentence\n",
    "            if next_word == '<eos>':\n",
    "                break\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            current_seq.append(next_idx)\n",
    "            \n",
    "            # Keep only last seq_length words for context\n",
    "            if len(current_seq) > seq_length:\n",
    "                current_seq = current_seq[-seq_length:]\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test text generation with incomplete sentences\n",
    "print(\"=\"*80)\n",
    "print(\"RNN TEXT GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, incomplete in enumerate(incomplete_sentences):\n",
    "    print(f\"\\nSentence {i}:\")\n",
    "    print(f\"  Incomplete: {' '.join(incomplete)}\")\n",
    "    \n",
    "    # Generate completion\n",
    "    completed = generate_text_rnn(rnn_model, incomplete, rnn_vocab, max_length=20, temperature=0.8, device=device)\n",
    "    \n",
    "    print(f\"  Completed:  {' '.join(completed)}\")\n",
    "    print(f\"  Length: {len(completed)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4df5d",
   "metadata": {},
   "source": [
    "# 3. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "800a8e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMLanguageModel(\n",
      "  (embedding): Embedding(28127, 128)\n",
      "  (lstm): LSTM(128, 128, num_layers=2)\n",
      "  (fc): Linear(in_features=128, out_features=28127, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "\n",
      "Total parameters: 7,492,831\n"
     ]
    }
   ],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=2, dropout=0.5):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        output, hidden = self.lstm(embedded, hidden)  # output: (batch_size, seq_length, hidden_dim)\n",
    "        \n",
    "        # Apply dropout and linear layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)  # (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = LSTMLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(lstm_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832a532",
   "metadata": {},
   "source": [
    "## Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fc26e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model ready for training!\n"
     ]
    }
   ],
   "source": [
    "def train_lstm(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the LSTM model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(inputs)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        # output: (batch_size, seq_length, vocab_size)\n",
    "        # targets: (batch_size, seq_length)\n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_correct += (predicted == targets).sum().item()\n",
    "        total_words += targets.size(0)\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'  Batch {batch_idx+1}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'Acc: {100*total_correct/total_words:.2f}%')\n",
    "    \n",
    "    avg_loss = total_loss / total_words\n",
    "    accuracy = total_correct / total_words\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_lstm(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate the LSTM model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, _ = model(inputs)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_words += targets.size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / total_words\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, accuracy, perplexity\n",
    "\n",
    "# Setup training\n",
    "lstm_model = lstm_model.to(device)\n",
    "criterion_lstm = nn.CrossEntropyLoss()\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"\\nModel ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54930cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING LSTM LANGUAGE MODEL\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 6.1631, Acc: 5.59%\n",
      "  Batch 100/5649, Loss: 6.1631, Acc: 5.59%\n",
      "  Batch 200/5649, Loss: 6.4798, Acc: 6.01%\n",
      "  Batch 200/5649, Loss: 6.4798, Acc: 6.01%\n",
      "  Batch 300/5649, Loss: 6.1385, Acc: 6.17%\n",
      "  Batch 300/5649, Loss: 6.1385, Acc: 6.17%\n",
      "  Batch 400/5649, Loss: 6.3080, Acc: 6.22%\n",
      "  Batch 400/5649, Loss: 6.3080, Acc: 6.22%\n",
      "  Batch 500/5649, Loss: 6.1776, Acc: 6.31%\n",
      "  Batch 500/5649, Loss: 6.1776, Acc: 6.31%\n",
      "  Batch 600/5649, Loss: 6.0722, Acc: 6.36%\n",
      "  Batch 600/5649, Loss: 6.0722, Acc: 6.36%\n",
      "  Batch 700/5649, Loss: 6.1147, Acc: 6.40%\n",
      "  Batch 700/5649, Loss: 6.1147, Acc: 6.40%\n",
      "  Batch 800/5649, Loss: 6.2504, Acc: 6.45%\n",
      "  Batch 800/5649, Loss: 6.2504, Acc: 6.45%\n",
      "  Batch 900/5649, Loss: 6.2047, Acc: 6.50%\n",
      "  Batch 900/5649, Loss: 6.2047, Acc: 6.50%\n",
      "  Batch 1000/5649, Loss: 6.1447, Acc: 6.51%\n",
      "  Batch 1000/5649, Loss: 6.1447, Acc: 6.51%\n",
      "  Batch 1100/5649, Loss: 6.0762, Acc: 6.51%\n",
      "  Batch 1100/5649, Loss: 6.0762, Acc: 6.51%\n",
      "  Batch 1200/5649, Loss: 6.0605, Acc: 6.53%\n",
      "  Batch 1200/5649, Loss: 6.0605, Acc: 6.53%\n",
      "  Batch 1300/5649, Loss: 6.1722, Acc: 6.55%\n",
      "  Batch 1300/5649, Loss: 6.1722, Acc: 6.55%\n",
      "  Batch 1400/5649, Loss: 6.0091, Acc: 6.57%\n",
      "  Batch 1400/5649, Loss: 6.0091, Acc: 6.57%\n",
      "  Batch 1500/5649, Loss: 6.0357, Acc: 6.61%\n",
      "  Batch 1500/5649, Loss: 6.0357, Acc: 6.61%\n",
      "  Batch 1600/5649, Loss: 6.3091, Acc: 6.69%\n",
      "  Batch 1600/5649, Loss: 6.3091, Acc: 6.69%\n",
      "  Batch 1700/5649, Loss: 5.7597, Acc: 6.80%\n",
      "  Batch 1700/5649, Loss: 5.7597, Acc: 6.80%\n",
      "  Batch 1800/5649, Loss: 5.6536, Acc: 6.93%\n",
      "  Batch 1800/5649, Loss: 5.6536, Acc: 6.93%\n",
      "  Batch 1900/5649, Loss: 5.7982, Acc: 7.09%\n",
      "  Batch 1900/5649, Loss: 5.7982, Acc: 7.09%\n",
      "  Batch 2000/5649, Loss: 5.7226, Acc: 7.30%\n",
      "  Batch 2000/5649, Loss: 5.7226, Acc: 7.30%\n",
      "  Batch 2100/5649, Loss: 5.4830, Acc: 7.51%\n",
      "  Batch 2100/5649, Loss: 5.4830, Acc: 7.51%\n",
      "  Batch 2200/5649, Loss: 5.6542, Acc: 7.71%\n",
      "  Batch 2200/5649, Loss: 5.6542, Acc: 7.71%\n",
      "  Batch 2300/5649, Loss: 5.3678, Acc: 7.93%\n",
      "  Batch 2300/5649, Loss: 5.3678, Acc: 7.93%\n",
      "  Batch 2400/5649, Loss: 5.4849, Acc: 8.15%\n",
      "  Batch 2400/5649, Loss: 5.4849, Acc: 8.15%\n",
      "  Batch 2500/5649, Loss: 5.6924, Acc: 8.36%\n",
      "  Batch 2500/5649, Loss: 5.6924, Acc: 8.36%\n",
      "  Batch 2600/5649, Loss: 5.3128, Acc: 8.57%\n",
      "  Batch 2600/5649, Loss: 5.3128, Acc: 8.57%\n",
      "  Batch 2700/5649, Loss: 5.3745, Acc: 8.80%\n",
      "  Batch 2700/5649, Loss: 5.3745, Acc: 8.80%\n",
      "  Batch 2800/5649, Loss: 5.2247, Acc: 9.03%\n",
      "  Batch 2800/5649, Loss: 5.2247, Acc: 9.03%\n",
      "  Batch 2900/5649, Loss: 5.1460, Acc: 9.26%\n",
      "  Batch 2900/5649, Loss: 5.1460, Acc: 9.26%\n",
      "  Batch 3000/5649, Loss: 5.4104, Acc: 9.49%\n",
      "  Batch 3000/5649, Loss: 5.4104, Acc: 9.49%\n",
      "  Batch 3100/5649, Loss: 5.1703, Acc: 9.71%\n",
      "  Batch 3100/5649, Loss: 5.1703, Acc: 9.71%\n",
      "  Batch 3200/5649, Loss: 5.0331, Acc: 9.94%\n",
      "  Batch 3200/5649, Loss: 5.0331, Acc: 9.94%\n",
      "  Batch 3300/5649, Loss: 5.2488, Acc: 10.16%\n",
      "  Batch 3300/5649, Loss: 5.2488, Acc: 10.16%\n",
      "  Batch 3400/5649, Loss: 5.0885, Acc: 10.37%\n",
      "  Batch 3400/5649, Loss: 5.0885, Acc: 10.37%\n",
      "  Batch 3500/5649, Loss: 5.1622, Acc: 10.58%\n",
      "  Batch 3500/5649, Loss: 5.1622, Acc: 10.58%\n",
      "  Batch 3600/5649, Loss: 5.2461, Acc: 10.78%\n",
      "  Batch 3600/5649, Loss: 5.2461, Acc: 10.78%\n",
      "  Batch 3700/5649, Loss: 5.2928, Acc: 10.98%\n",
      "  Batch 3700/5649, Loss: 5.2928, Acc: 10.98%\n",
      "  Batch 3800/5649, Loss: 5.0342, Acc: 11.17%\n",
      "  Batch 3800/5649, Loss: 5.0342, Acc: 11.17%\n",
      "  Batch 3900/5649, Loss: 5.1445, Acc: 11.36%\n",
      "  Batch 3900/5649, Loss: 5.1445, Acc: 11.36%\n",
      "  Batch 4000/5649, Loss: 5.0444, Acc: 11.53%\n",
      "  Batch 4000/5649, Loss: 5.0444, Acc: 11.53%\n",
      "  Batch 4100/5649, Loss: 5.0009, Acc: 11.71%\n",
      "  Batch 4100/5649, Loss: 5.0009, Acc: 11.71%\n",
      "  Batch 4200/5649, Loss: 4.8910, Acc: 11.88%\n",
      "  Batch 4200/5649, Loss: 4.8910, Acc: 11.88%\n",
      "  Batch 4300/5649, Loss: 4.7907, Acc: 12.05%\n",
      "  Batch 4300/5649, Loss: 4.7907, Acc: 12.05%\n",
      "  Batch 4400/5649, Loss: 4.9672, Acc: 12.22%\n",
      "  Batch 4400/5649, Loss: 4.9672, Acc: 12.22%\n",
      "  Batch 4500/5649, Loss: 4.9882, Acc: 12.37%\n",
      "  Batch 4500/5649, Loss: 4.9882, Acc: 12.37%\n",
      "  Batch 4600/5649, Loss: 5.0491, Acc: 12.52%\n",
      "  Batch 4600/5649, Loss: 5.0491, Acc: 12.52%\n",
      "  Batch 4700/5649, Loss: 4.8231, Acc: 12.67%\n",
      "  Batch 4700/5649, Loss: 4.8231, Acc: 12.67%\n",
      "  Batch 4800/5649, Loss: 4.6538, Acc: 12.82%\n",
      "  Batch 4800/5649, Loss: 4.6538, Acc: 12.82%\n",
      "  Batch 4900/5649, Loss: 5.1635, Acc: 12.96%\n",
      "  Batch 4900/5649, Loss: 5.1635, Acc: 12.96%\n",
      "  Batch 5000/5649, Loss: 4.6062, Acc: 13.10%\n",
      "  Batch 5000/5649, Loss: 4.6062, Acc: 13.10%\n",
      "  Batch 5100/5649, Loss: 4.5363, Acc: 13.23%\n",
      "  Batch 5100/5649, Loss: 4.5363, Acc: 13.23%\n",
      "  Batch 5200/5649, Loss: 4.7864, Acc: 13.36%\n",
      "  Batch 5200/5649, Loss: 4.7864, Acc: 13.36%\n",
      "  Batch 5300/5649, Loss: 4.8986, Acc: 13.49%\n",
      "  Batch 5300/5649, Loss: 4.8986, Acc: 13.49%\n",
      "  Batch 5400/5649, Loss: 4.6274, Acc: 13.62%\n",
      "  Batch 5400/5649, Loss: 4.6274, Acc: 13.62%\n",
      "  Batch 5500/5649, Loss: 4.6937, Acc: 13.74%\n",
      "  Batch 5500/5649, Loss: 4.6937, Acc: 13.74%\n",
      "  Batch 5600/5649, Loss: 4.8995, Acc: 13.85%\n",
      "  Batch 5600/5649, Loss: 4.8995, Acc: 13.85%\n",
      "Train Loss: 5.4606, Train Accuracy: 13.91%\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------------------------------------\n",
      "Train Loss: 5.4606, Train Accuracy: 13.91%\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.2771, Acc: 20.89%\n",
      "  Batch 100/5649, Loss: 4.2771, Acc: 20.89%\n",
      "  Batch 200/5649, Loss: 4.6325, Acc: 20.95%\n",
      "  Batch 200/5649, Loss: 4.6325, Acc: 20.95%\n",
      "  Batch 300/5649, Loss: 4.7432, Acc: 20.88%\n",
      "  Batch 300/5649, Loss: 4.7432, Acc: 20.88%\n",
      "  Batch 400/5649, Loss: 4.5333, Acc: 20.89%\n",
      "  Batch 400/5649, Loss: 4.5333, Acc: 20.89%\n",
      "  Batch 500/5649, Loss: 4.4548, Acc: 20.87%\n",
      "  Batch 500/5649, Loss: 4.4548, Acc: 20.87%\n",
      "  Batch 600/5649, Loss: 4.3491, Acc: 20.87%\n",
      "  Batch 600/5649, Loss: 4.3491, Acc: 20.87%\n",
      "  Batch 700/5649, Loss: 4.8508, Acc: 20.89%\n",
      "  Batch 700/5649, Loss: 4.8508, Acc: 20.89%\n",
      "  Batch 800/5649, Loss: 4.5187, Acc: 20.90%\n",
      "  Batch 800/5649, Loss: 4.5187, Acc: 20.90%\n",
      "  Batch 900/5649, Loss: 4.6446, Acc: 20.93%\n",
      "  Batch 900/5649, Loss: 4.6446, Acc: 20.93%\n",
      "  Batch 1000/5649, Loss: 4.5840, Acc: 20.96%\n",
      "  Batch 1000/5649, Loss: 4.5840, Acc: 20.96%\n",
      "  Batch 1100/5649, Loss: 4.4667, Acc: 20.97%\n",
      "  Batch 1100/5649, Loss: 4.4667, Acc: 20.97%\n",
      "  Batch 1200/5649, Loss: 4.4480, Acc: 21.00%\n",
      "  Batch 1200/5649, Loss: 4.4480, Acc: 21.00%\n",
      "  Batch 1300/5649, Loss: 4.7400, Acc: 21.03%\n",
      "  Batch 1300/5649, Loss: 4.7400, Acc: 21.03%\n",
      "  Batch 1400/5649, Loss: 4.7002, Acc: 21.03%\n",
      "  Batch 1400/5649, Loss: 4.7002, Acc: 21.03%\n",
      "  Batch 1500/5649, Loss: 4.7256, Acc: 21.05%\n",
      "  Batch 1500/5649, Loss: 4.7256, Acc: 21.05%\n",
      "  Batch 1600/5649, Loss: 4.6616, Acc: 21.06%\n",
      "  Batch 1600/5649, Loss: 4.6616, Acc: 21.06%\n",
      "  Batch 1700/5649, Loss: 4.7136, Acc: 21.06%\n",
      "  Batch 1700/5649, Loss: 4.7136, Acc: 21.06%\n",
      "  Batch 1800/5649, Loss: 4.6837, Acc: 21.10%\n",
      "  Batch 1800/5649, Loss: 4.6837, Acc: 21.10%\n",
      "  Batch 1900/5649, Loss: 4.5386, Acc: 21.11%\n",
      "  Batch 1900/5649, Loss: 4.5386, Acc: 21.11%\n",
      "  Batch 2000/5649, Loss: 4.4570, Acc: 21.12%\n",
      "  Batch 2000/5649, Loss: 4.4570, Acc: 21.12%\n",
      "  Batch 2100/5649, Loss: 4.3808, Acc: 21.14%\n",
      "  Batch 2100/5649, Loss: 4.3808, Acc: 21.14%\n",
      "  Batch 2200/5649, Loss: 4.5303, Acc: 21.16%\n",
      "  Batch 2200/5649, Loss: 4.5303, Acc: 21.16%\n",
      "  Batch 2300/5649, Loss: 4.5545, Acc: 21.18%\n",
      "  Batch 2300/5649, Loss: 4.5545, Acc: 21.18%\n",
      "  Batch 2400/5649, Loss: 4.7416, Acc: 21.22%\n",
      "  Batch 2400/5649, Loss: 4.7416, Acc: 21.22%\n",
      "  Batch 2500/5649, Loss: 4.5834, Acc: 21.24%\n",
      "  Batch 2500/5649, Loss: 4.5834, Acc: 21.24%\n",
      "  Batch 2600/5649, Loss: 4.5058, Acc: 21.25%\n",
      "  Batch 2600/5649, Loss: 4.5058, Acc: 21.25%\n",
      "  Batch 2700/5649, Loss: 4.1451, Acc: 21.27%\n",
      "  Batch 2700/5649, Loss: 4.1451, Acc: 21.27%\n",
      "  Batch 2800/5649, Loss: 4.8054, Acc: 21.29%\n",
      "  Batch 2800/5649, Loss: 4.8054, Acc: 21.29%\n",
      "  Batch 2900/5649, Loss: 4.4984, Acc: 21.32%\n",
      "  Batch 2900/5649, Loss: 4.4984, Acc: 21.32%\n",
      "  Batch 3000/5649, Loss: 4.6154, Acc: 21.34%\n",
      "  Batch 3000/5649, Loss: 4.6154, Acc: 21.34%\n",
      "  Batch 3100/5649, Loss: 4.5060, Acc: 21.35%\n",
      "  Batch 3100/5649, Loss: 4.5060, Acc: 21.35%\n",
      "  Batch 3200/5649, Loss: 4.4605, Acc: 21.36%\n",
      "  Batch 3200/5649, Loss: 4.4605, Acc: 21.36%\n",
      "  Batch 3300/5649, Loss: 4.6868, Acc: 21.37%\n",
      "  Batch 3300/5649, Loss: 4.6868, Acc: 21.37%\n",
      "  Batch 3400/5649, Loss: 4.3550, Acc: 21.39%\n",
      "  Batch 3400/5649, Loss: 4.3550, Acc: 21.39%\n",
      "  Batch 3500/5649, Loss: 4.5198, Acc: 21.41%\n",
      "  Batch 3500/5649, Loss: 4.5198, Acc: 21.41%\n",
      "  Batch 3600/5649, Loss: 4.4180, Acc: 21.44%\n",
      "  Batch 3600/5649, Loss: 4.4180, Acc: 21.44%\n",
      "  Batch 3700/5649, Loss: 4.3640, Acc: 21.46%\n",
      "  Batch 3700/5649, Loss: 4.3640, Acc: 21.46%\n",
      "  Batch 3800/5649, Loss: 4.4263, Acc: 21.47%\n",
      "  Batch 3800/5649, Loss: 4.4263, Acc: 21.47%\n",
      "  Batch 3900/5649, Loss: 4.2543, Acc: 21.48%\n",
      "  Batch 3900/5649, Loss: 4.2543, Acc: 21.48%\n",
      "  Batch 4000/5649, Loss: 4.6955, Acc: 21.50%\n",
      "  Batch 4000/5649, Loss: 4.6955, Acc: 21.50%\n",
      "  Batch 4100/5649, Loss: 4.3507, Acc: 21.51%\n",
      "  Batch 4100/5649, Loss: 4.3507, Acc: 21.51%\n",
      "  Batch 4200/5649, Loss: 4.2890, Acc: 21.53%\n",
      "  Batch 4200/5649, Loss: 4.2890, Acc: 21.53%\n",
      "  Batch 4300/5649, Loss: 4.4899, Acc: 21.55%\n",
      "  Batch 4300/5649, Loss: 4.4899, Acc: 21.55%\n",
      "  Batch 4400/5649, Loss: 4.5620, Acc: 21.56%\n",
      "  Batch 4400/5649, Loss: 4.5620, Acc: 21.56%\n",
      "  Batch 4500/5649, Loss: 4.5639, Acc: 21.58%\n",
      "  Batch 4500/5649, Loss: 4.5639, Acc: 21.58%\n",
      "  Batch 4600/5649, Loss: 4.3707, Acc: 21.59%\n",
      "  Batch 4600/5649, Loss: 4.3707, Acc: 21.59%\n",
      "  Batch 4700/5649, Loss: 4.2918, Acc: 21.61%\n",
      "  Batch 4700/5649, Loss: 4.2918, Acc: 21.61%\n",
      "  Batch 4800/5649, Loss: 4.6304, Acc: 21.62%\n",
      "  Batch 4800/5649, Loss: 4.6304, Acc: 21.62%\n",
      "  Batch 4900/5649, Loss: 4.6162, Acc: 21.63%\n",
      "  Batch 4900/5649, Loss: 4.6162, Acc: 21.63%\n",
      "  Batch 5000/5649, Loss: 4.5806, Acc: 21.64%\n",
      "  Batch 5000/5649, Loss: 4.5806, Acc: 21.64%\n",
      "  Batch 5100/5649, Loss: 4.3761, Acc: 21.64%\n",
      "  Batch 5100/5649, Loss: 4.3761, Acc: 21.64%\n",
      "  Batch 5200/5649, Loss: 4.5568, Acc: 21.66%\n",
      "  Batch 5200/5649, Loss: 4.5568, Acc: 21.66%\n",
      "  Batch 5300/5649, Loss: 4.5163, Acc: 21.68%\n",
      "  Batch 5300/5649, Loss: 4.5163, Acc: 21.68%\n",
      "  Batch 5400/5649, Loss: 4.4795, Acc: 21.69%\n",
      "  Batch 5400/5649, Loss: 4.4795, Acc: 21.69%\n",
      "  Batch 5500/5649, Loss: 4.5706, Acc: 21.69%\n",
      "  Batch 5500/5649, Loss: 4.5706, Acc: 21.69%\n",
      "  Batch 5600/5649, Loss: 4.4789, Acc: 21.70%\n",
      "  Batch 5600/5649, Loss: 4.4789, Acc: 21.70%\n",
      "Train Loss: 4.5533, Train Accuracy: 21.71%\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------------------------------------\n",
      "Train Loss: 4.5533, Train Accuracy: 21.71%\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------------------------------------\n",
      "  Batch 100/5649, Loss: 4.2784, Acc: 22.43%\n",
      "  Batch 100/5649, Loss: 4.2784, Acc: 22.43%\n",
      "  Batch 200/5649, Loss: 4.3445, Acc: 22.46%\n",
      "  Batch 200/5649, Loss: 4.3445, Acc: 22.46%\n",
      "  Batch 300/5649, Loss: 4.2828, Acc: 22.47%\n",
      "  Batch 300/5649, Loss: 4.2828, Acc: 22.47%\n",
      "  Batch 400/5649, Loss: 4.0967, Acc: 22.48%\n",
      "  Batch 400/5649, Loss: 4.0967, Acc: 22.48%\n",
      "  Batch 500/5649, Loss: 4.4969, Acc: 22.54%\n",
      "  Batch 500/5649, Loss: 4.4969, Acc: 22.54%\n",
      "  Batch 600/5649, Loss: 4.4361, Acc: 22.55%\n",
      "  Batch 600/5649, Loss: 4.4361, Acc: 22.55%\n",
      "  Batch 700/5649, Loss: 4.2185, Acc: 22.49%\n",
      "  Batch 700/5649, Loss: 4.2185, Acc: 22.49%\n",
      "  Batch 800/5649, Loss: 4.3859, Acc: 22.48%\n",
      "  Batch 800/5649, Loss: 4.3859, Acc: 22.48%\n",
      "  Batch 900/5649, Loss: 4.7437, Acc: 22.46%\n",
      "  Batch 900/5649, Loss: 4.7437, Acc: 22.46%\n",
      "  Batch 1000/5649, Loss: 4.2739, Acc: 22.51%\n",
      "  Batch 1000/5649, Loss: 4.2739, Acc: 22.51%\n",
      "  Batch 1100/5649, Loss: 4.3944, Acc: 22.51%\n",
      "  Batch 1100/5649, Loss: 4.3944, Acc: 22.51%\n",
      "  Batch 1200/5649, Loss: 4.4617, Acc: 22.53%\n",
      "  Batch 1200/5649, Loss: 4.4617, Acc: 22.53%\n",
      "  Batch 1300/5649, Loss: 4.1459, Acc: 22.53%\n",
      "  Batch 1300/5649, Loss: 4.1459, Acc: 22.53%\n",
      "  Batch 1400/5649, Loss: 4.5911, Acc: 22.54%\n",
      "  Batch 1400/5649, Loss: 4.5911, Acc: 22.54%\n",
      "  Batch 1500/5649, Loss: 4.3575, Acc: 22.55%\n",
      "  Batch 1500/5649, Loss: 4.3575, Acc: 22.55%\n",
      "  Batch 1600/5649, Loss: 4.3656, Acc: 22.53%\n",
      "  Batch 1600/5649, Loss: 4.3656, Acc: 22.53%\n",
      "  Batch 1700/5649, Loss: 4.0575, Acc: 22.56%\n",
      "  Batch 1700/5649, Loss: 4.0575, Acc: 22.56%\n"
     ]
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "num_epochs = 10\n",
    "\n",
    "# Store training history\n",
    "lstm_history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING LSTM LANGUAGE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_lstm(lstm_model, train_loader, criterion_lstm, optimizer_lstm, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # Store history\n",
    "    lstm_history['train_loss'].append(train_loss)\n",
    "    lstm_history['train_acc'].append(train_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6c419",
   "metadata": {},
   "source": [
    "## Plot LSTM Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a015d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "axes[0].plot(epochs_range, lstm_history['train_loss'], 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('LSTM Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Accuracy\n",
    "axes[1].plot(epochs_range, [acc * 100 for acc in lstm_history['train_acc']], 'g-o', label='Training Accuracy', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('LSTM Training Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40c478",
   "metadata": {},
   "source": [
    "## Evaluate the LSTM model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss_lstm, test_acc_lstm, test_perplexity_lstm = evaluate_lstm(rnn_model, test_loader, criterion, device)\n",
    "print(f\"Test Accuracy: {test_acc_lstm*100:.2f}%, Perplexity: {test_perplexity_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584050f",
   "metadata": {},
   "source": [
    "## Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfdd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_lstm(model, start_words, vocab, max_length=20, temperature=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using the trained LSTM model\n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        start_words: List of starting words\n",
    "        vocab: Vocabulary dictionary\n",
    "        max_length: Maximum length of generated sequence\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        device: Device to run on\n",
    "    Returns:\n",
    "        Generated text as a list of words\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create reverse vocabulary\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    # Convert start words to indices\n",
    "    current_seq = [vocab.get(word, vocab['<unk>']) for word in start_words]\n",
    "    generated = list(start_words)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_words)):\n",
    "            # Prepare input\n",
    "            input_tensor = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Get model prediction\n",
    "            output, _ = model(input_tensor)\n",
    "            \n",
    "            # Get last word prediction\n",
    "            logits = output[0, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "            \n",
    "            # Sample next word\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            next_word = idx_to_word.get(next_idx, '<unk>')\n",
    "            \n",
    "            # Stop if end of sentence\n",
    "            if next_word == '<eos>':\n",
    "                break\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            current_seq.append(next_idx)\n",
    "            \n",
    "            # Keep only last seq_length words for context\n",
    "            if len(current_seq) > seq_length:\n",
    "                current_seq = current_seq[-seq_length:]\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test text generation with incomplete sentences\n",
    "print(\"=\"*80)\n",
    "print(\"LSTM TEXT GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, incomplete in enumerate(incomplete_sentences[:5], 1):\n",
    "    print(f\"\\nSentence {i}:\")\n",
    "    print(f\"  Incomplete: {' '.join(incomplete)}\")\n",
    "    \n",
    "    # Generate completion\n",
    "    completed = generate_text_lstm(lstm_model, incomplete, rnn_vocab, max_length=20, temperature=0.8, device=device)\n",
    "    \n",
    "    print(f\"  Completed:  {' '.join(completed)}\")\n",
    "    print(f\"  Length: {len(completed)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a5116",
   "metadata": {},
   "source": [
    "## Model Comparison: N-gram vs RNN vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final performance of all three models\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. N-GRAM MODEL (Trigram):\")\n",
    "print(f\"   Test Accuracy: {trigram_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n2. RNN MODEL:\")\n",
    "if len(history['train_loss']) > 0:\n",
    "    print(f\"   Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"   Training Accuracy: {history['train_acc'][-1] * 100:.2f}%\")\n",
    "    print(f\"   Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "    print(f\"   Test Perplexity: {test_perplexity:.2f}\")\n",
    "else:\n",
    "    print(\"   Not trained yet\")\n",
    "\n",
    "print(\"\\n3. LSTM MODEL:\")\n",
    "if len(lstm_history['train_loss']) > 0:\n",
    "    print(f\"   Training Loss: {lstm_history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"   Training Accuracy: {lstm_history['train_acc'][-1] * 100:.2f}%\")\n",
    "    print(f\"   Test Accuracy: {test_acc_lstm * 100:.2f}%\")\n",
    "    print(f\"   Test Perplexity: {test_perplexity_lstm:.2f}\")\n",
    "else:\n",
    "    print(\"   Not trained yet\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
